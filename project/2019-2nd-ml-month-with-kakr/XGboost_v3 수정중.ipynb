{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "def timer(start_time=None):\n",
    "    if not start_time:\n",
    "        start_time = datetime.now()\n",
    "        return start_time\n",
    "    elif start_time:\n",
    "        thour, temp_sec = divmod((datetime.now() - start_time).total_seconds(), 3600)\n",
    "        tmin, tsec = divmod(temp_sec, 60)\n",
    "        print('\\n Time taken: %i hours %i minutes and %s seconds.' % (thour, tmin, round(tsec, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"train.csv\")\n",
    "df_test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 수정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [df_train,df_test]:\n",
    "    # df['date'] = df['date'].apply(lambda x: x[0:8])\n",
    "    # 일짜까지 되있는걸 월까지\n",
    "    df['date'] = df['date'].apply(lambda x: x[0:6])\n",
    "    df['data_y'] = ''\n",
    "    df['data_m'] = ''\n",
    "    df['data_y'] = df['date'].apply(lambda x : x[0:4])\n",
    "    df['data_m'] = df['date'].apply(lambda x : x[4:6])\n",
    "    df['yr_renovated'] = df['yr_renovated'].apply(lambda x: np.nan if x == 0 else x)\n",
    "    df['yr_renovated'] = df['yr_renovated'].fillna(df['yr_built'])\n",
    "for df in [df_train]:\n",
    "    # 1000단위 대로 반올림\n",
    "    df['price'] = df['price'].apply(lambda x: round(x, -3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [df_train,df_test]:\n",
    "    df['total_rooms'] = df['bedrooms'] + df['bathrooms']\n",
    "    df['sqft_ratio'] = df['sqft_living'] / df['sqft_lot']\n",
    "    df['sqft_total_size'] = df['sqft_above'] + df['sqft_basement']\n",
    "    df['sqft_ratio15'] = df['sqft_living15'] / df['sqft_lot15'] \n",
    "    df['is_renovated'] = df['yr_renovated'] - df['yr_built']\n",
    "    df['is_renovated'] = df['is_renovated'].apply(lambda x: 0 if x == 0 else 1)\n",
    "    df['date'] = df['date'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['per_price'] = df_train['price']/df_train['sqft_total_size']\n",
    "zipcode_price = df_train.groupby(['zipcode'])['per_price'].agg({'mean','var'}).reset_index()\n",
    "df_train = pd.merge(df_train,zipcode_price,how='left',on='zipcode')\n",
    "df_test = pd.merge(df_test,zipcode_price,how='left',on='zipcode')\n",
    "\n",
    "for df in [df_train,df_test]:\n",
    "    df['mean'] = df['mean'] * df['sqft_total_size']\n",
    "    df['var'] = df['var'] * df['sqft_total_size']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 라벨 인코더"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "# LE = LabelEncoder()\n",
    "# LE.fit(df_train[['date']])\n",
    "# df_train[['date']] = LE.transform(df_train[['date']])\n",
    "# LE.fit(df_train[['zipcode']])\n",
    "# df_train[['zipcode']] = LE.transform(df_train[['zipcode']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 상호작용변수 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geogege(data):\n",
    "    data['zipcode'] = data['zipcode'].astype(str)  \n",
    "    data['zipcode-3'] = data['zipcode'].apply(lambda x : str(x[2:3])).astype(int)\n",
    "    data['zipcode-4'] = data['zipcode'].apply(lambda x : str(x[3:4])).astype(int)\n",
    "    data['zipcode-5'] = data['zipcode'].apply(lambda x : str(x[4:5])).astype(int)\n",
    "    data['zipcode-34'] = data['zipcode'].apply(lambda x : str(x[2:4])).astype(int)\n",
    "    data['zipcode-45'] = data['zipcode'].apply(lambda x : str(x[3:5])).astype(int)\n",
    "    data['zipcode-35'] = data['zipcode'].apply(lambda x : str(x[2:5])).astype(int)\n",
    "#     data.drop('zipcode', axis=1, inplace=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppcca1(data):\n",
    "    pca2 = PCA(n_components=2)\n",
    "    coord_test = data[['lat','long']]\n",
    "    \n",
    "    principalComponents_updated_test = pca2.transform(coord_test)\n",
    "    data['coord_pca1']= ''\n",
    "    data['coord_pca2']= ''\n",
    "    data['coord_pca1']= principalComponents_updated_test[:, 0]\n",
    "    data['coord_pca2']= principalComponents_updated_test[:, 1]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppcca2(data):\n",
    "    pca1 = PCA(n_components=2)\n",
    "\n",
    "    principalComponents_updated_test = pca1.transform(data)\n",
    "    data['pca1']= ''\n",
    "    data['pca2']= ''\n",
    "    data['pca1']= principalComponents_updated_test[:, 0]\n",
    "    data['pca2']= principalComponents_updated_test[:, 1]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_length = len(df_train)\n",
    "concat_dataset = pd.concat(objs=[df_train, df_test], axis=0)\n",
    "\n",
    "# ZIP divide\n",
    "concat_dataset = geogege(concat_dataset)\n",
    "\n",
    "# LabelEncoder\n",
    "LE = LabelEncoder()\n",
    "# LE.fit(df_train[['date']])\n",
    "# concat_dataset[['date']] = LE.transform(concat_dataset[['date']])\n",
    "# LE.fit(df_train[['zipcode']])\n",
    "# concat_dataset[['zipcode']] = LE.transform(concat_dataset[['zipcode']])\n",
    "concat_dataset['date_zipcode'] = concat_dataset['date'].map(str) + \"_\" + concat_dataset['zipcode'].map(str)\n",
    "LE.fit(concat_dataset[['date_zipcode']])\n",
    "concat_dataset[['date_zipcode']] = LE.transform(concat_dataset[['date_zipcode']])\n",
    "\n",
    "# PCA\n",
    "# concat_dataset = ppcca1(concat_dataset)\n",
    "# concat_dataset = ppcca2(concat_dataset)\n",
    "\n",
    "\n",
    "# concat_dataset[concat_dataset.columns.difference(['per_price','price'])]\n",
    "\n",
    "# df_train = concat_dataset[:df_train_length]\n",
    "# df_test = concat_dataset[df_train_length:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got scalar array instead:\narray=PCA(copy=True, iterated_power='auto', n_components=6, random_state=None,\n  svd_solver='auto', tol=0.0, whiten=False).\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-73ccf067e8ea>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mpca\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mpca1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpca\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconcat_dataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mconcat_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdifference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'per_price'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'price'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdf3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpca\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpca1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\base.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    126\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'mean_'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'components_'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mall_or_any\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean_\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    543\u001b[0m                     \u001b[1;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m                     \u001b[1;34m\"your data has a single feature or array.reshape(1, -1) \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m                     \"if it contains a single sample.\".format(array))\n\u001b[0m\u001b[0;32m    546\u001b[0m             \u001b[1;31m# If input is 1D raise error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 2D array, got scalar array instead:\narray=PCA(copy=True, iterated_power='auto', n_components=6, random_state=None,\n  svd_solver='auto', tol=0.0, whiten=False).\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components=6)\n",
    "pca1 = pca.fit(concat_dataset[concat_dataset.columns.difference(['per_price','price'])])\n",
    "df3 = pca.transform(pca1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "transform() missing 1 required positional argument: 'X'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-f04d97658716>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpca1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: transform() missing 1 required positional argument: 'X'"
     ]
    }
   ],
   "source": [
    "pca1.transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca2 = PCA(n_components=2)\n",
    "coord_test = concat_dataset[['lat','long']]\n",
    "\n",
    "principalComponents_updated_test = pca2.transform(coord_test)\n",
    "df3['coord_pca1']= ''\n",
    "df3['coord_pca2']= ''\n",
    "df3['coord_pca1']= principalComponents_updated_test[:, 0]\n",
    "df3['coord_pca2']= principalComponents_updated_test[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=4, random_state=None,\n",
       "  svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "X_train = df3[:df_train_length]\n",
    "y_train\n",
    "df_test = df3[df_train_length:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 월별 + ZIPCODE를 활용한 더미변수 추가하여 테스트\n",
    "위도 + 경도로 위치값을 대신하고 싶었지만 어떻게 구역을 나눠야 할지 모르기때문에 ZIPCODE로 대신함"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_train_length = len(df_train)\n",
    "concat_dataset = pd.concat(objs=[df_train, df_test], axis=0)\n",
    "\n",
    "concat_dataset['date_zipcode'] = concat_dataset['date'].map(str) + \"_\" + concat_dataset['zipcode'].map(str)\n",
    "dataset_preprocessed = pd.get_dummies(concat_dataset, columns=['date', 'zipcode','date_zipcode'])\n",
    "\n",
    "df_train = dataset_preprocessed[:df_train_length]\n",
    "df_test = dataset_preprocessed[df_train_length:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGboost"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score, cross_val_predict\n",
    "import xgboost as xgb\n",
    "\n",
    "train_columns = [c for c in df_train.columns if c not in ['id','price','per_price']]\n",
    "y_reg = df_train['price']\n",
    "\n",
    "\n",
    "\n",
    "xgb_params = {\n",
    "    'learning_rate': 0.01\n",
    "    ,'n_estimators': 10000\n",
    "    ,'max_depth': 7\n",
    "    ,'subsample': 0.9\n",
    "    ,'num_boost_round': 3000\n",
    "    ,'early_stopping_rounds': 500\n",
    "#     ,'eval_metric': 'rmse'\n",
    "#     ,'objective': 'reg:linear'\n",
    "    ,'seed': 2019\n",
    "}\n",
    "\n",
    "model= xgb.XGBRegressor(\n",
    "    # eta [default=0.3, alias: learning_rate] 학습율\n",
    "    learning_rate=0.005,\n",
    "    # 모형 갯수 default=100\n",
    "    n_estimators=80001,\n",
    "    random_state=2019,\n",
    "    # 학습 횟수\n",
    "    num_boost_round=10000, \n",
    "    # overfitting 방지\n",
    "    early_stopping_rounds=5,\n",
    "    verbose_eval=3000, \n",
    "    show_stdv=False,\n",
    "    max_depth=7, \n",
    "    nthread=-1,\n",
    "    tree_method='gpu_hist'\n",
    ")\n",
    "\n",
    "#prepare fit model with cross-validation\n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=2019)\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(df_train)):\n",
    "    X_train,X_validation=df_train.iloc[trn_idx][train_columns],df_train.iloc[val_idx][train_columns]\n",
    "    y_train,y_validation=y_reg.iloc[trn_idx],y_reg.iloc[val_idx]\n",
    "    \n",
    "    dtrain = xgb.DMatrix(X_train)\n",
    "    dtarget=xgb.DMatrix(y_train)\n",
    "    dtest = xgb.DMatrix(X_validation)\n",
    "    dtest_target=xgb.DMatrix(y_validation)\n",
    "    \n",
    "    model.fit(X_train,y_train\n",
    "              ,eval_set=[(X_train,y_train),(X_validation,y_validation)]\n",
    "              ,eval_metric='rmse'\n",
    "              # log 얼마단위로 찍을건지\n",
    "              ,verbose=2000\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score, cross_val_predict\n",
    "import xgboost as xgb\n",
    "\n",
    "# X_train = [c for c in df_train.columns if c not in ['id','price','per_price']]\n",
    "# y_train = df_train['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= xgb.XGBRegressor(\n",
    "    n_estimators=50000,\n",
    "    num_round_boost=500,\n",
    "    verbosity=0,\n",
    "    reg_lambda=10,\n",
    "    reg_alpha=0.01,\n",
    "    learning_rate=0.005,\n",
    "    seed=2019,\n",
    "    colsample_bytree=0.8,\n",
    "    colsample_bylevel=0.8,\n",
    "    subsample=0.9,\n",
    "    n_jobs=-1,\n",
    "    gamma=0.005,\n",
    "    nthread=-1\n",
    ")\n",
    "\n",
    "model.fit(X_train,y_train, verbose=False, eval_set=watchlist,\n",
    "              eval_metric='rmse',\n",
    "              verbose=2000,\n",
    "              early_stopping_rounds=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score=mse(np.exp(model.predict(X_test)),np.exp(y_test))**0.5\n",
    "pred=np.exp(model.predict(Y_test))\n",
    "\n",
    "print(\"RMSE unseen : {}\".format(xgb_score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
