데이터 전처리	
	dummy_X = np.array(pd.get_dummies(dummy_X,drop_first=False)) # One-Hot Encoding. 명목형변수 수치형데이터로 변환
	from sklearn import metrics, preprocessing
	LE = preprocessing.LabelEncoder()
	Y = LE.fit_transform(Y)		# yes or no 같은 레이블을 숫자로 변환
	IPT = preprocessing.Imputer()
	X = IPT.fit_transform(X)		# NaN이 있으면 채워 넣음
	X = preprocessing.scale(X)	# 표준화
	
테스트 및 트레이닝 데이터 분리
	from sklearn.model_selection import train_test_split
	X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=num, random_state=num) #random_state는 random seed라는데 없어도 됨. 일반적으로 test_size 0.3으로 잡음

교차검증(Cross Validation)
	데이터는 트레이닝과 테스팅 세트로 구분되며 모형을 만드는 과정에서는 절대 테스팅 데이터세트를 사용하면 안됨
	트레이닝데이터를 더 작은 트레이닝 세트와 검증용 세트로 분활하여 학습시킴. 이 과정을 몇번 반복함
	분활 방법
		K-fold: 동일한사이즈K개의파트로나누어서순차적으로적용.
		Leave One Out: 단하나의관측값만교차검증용으로남겨둔다.
	from sklearn.model_selection import train_test_split,RandomizedSearchCV, GridSearchCV
	estimator_grid = np.arange(1, 30, 5)
	depth_grid = np.arange(1, 10, 2)
	parameters = {'n_estimators': estimator_grid, 'max_depth': depth_grid}
	gridCV = GridSearchCV(사용할머신러닝함수, param_grid=parameters, cv=10) # cv는 교차검증 10이면 10Fold

머신러닝
	지도학습과 시험
		진정한 학습은 암기한 내용을 되새김질 하는게 아님
		현실에 적용할 수 있도록 일반화가 검증되어야 함
		일반화를 검증하기 위해 학습(train)된 모형을 시험(test)함
		시험의 결과는 '오류'로 평가
	오류의 유형
		편향오류(bias error)
		과소적합 오류(underfitting error)
	크게 트리형과 회귀형 두개가 있음
	선형회귀
	  장점
		수치예측의 가장 보편적인 방법
		통계적, 이론적 배경 견고
		트레이닝이 빠름
	  단점
		여러가지 가정 전체: 선형성, 독립성 등
		외상치에 민감
		과적합 문제 쉽게 발생
	  코드
		from sklearn.linear_model import LinearRegression
		lm = LinearRegression()	#초기화
		lm.fit(X,Y)	#학습
		lm.coef_	#가중치확인
		lm.intercept_	#절편확인
		lm.score(X,Y)	#결정계수 확인
		X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=5)	#학습 및 시험데이터 분류
		lm.fit(X_train,Y_train)	#학습, 여기서 Y는 X변수들에 의한 결과값 Y= B0 + B1X1 + ... + BnXn
		#결과 확인
		Y_pred_train = lm.predict(X_train)
		Y_pred_test = lm.predict(X_test)
		#결과값 출력
		print('Training MSE is: ' + str(metrics.mean_squared_error(Y_train, Y_pred_train)))
		print('Testing MSE is: ' + str(metrics.mean_squared_error(Y_test, Y_pred_test)))
		print('-'*50)
		print('Training RMSE is: ' + str(np.sqrt(metrics.mean_squared_error(Y_train, Y_pred_train))))
		print('Testing RMSE is: ' + str(np.sqrt(metrics.mean_squared_error(Y_test, Y_pred_test))))
		# 명목형변수 생성 방법
		pd.get_dummies(데이터.컬럼이름 , drop_first=True|False True면 컬럼1개 제외 False면 제외안함, prefix='결과컬럼앞에 붙일거. 없어도됨')
		# 더미변수 생성 방법
		더비변수명 = pd.get_dummies(데이터.컬럼이름, drop_first=True)
		X = 데이터[['컬럼이름']].join(더비변수명)
			# 교차항
			poly = PolynomialFeatures(interaction_only=True,include_bias=False)
			X = pd.DataFrame(poly.fit_transform(X), columns = list(X_header) + ['교차항명'])
			사실상 poly.fit_transform(X) 이게 다 해버림
			Focus 선형회귀
			Ridge 회귀: 
			앱실론 최소화
			과적합(overfitting)의 상황이 의심될 때 유효
			람다는 양의 상수로써 크면 클수록 분산오류를 줄이며 편향오류를 증가시킴
			절대값은 억제되지만 정확하게 0이되지 않음
			Lasso 회귀
			가중치 최소화
			Ridge와 마찬가지로 과적합(overfitting)의 상황이 의심될 때 유효
			람다가 과하게 크면 편향오류의 증가가 분산오류의 감소를 상쇄하고도 남을 수 있으니 주의
			최귀계수가 정확하게 0이 될 수 있음
	로지스틱 회귀
	  장점
		개념과 이론이 단순함
		노이즈에 비교적 둔감
		트레이닝이 빠름
	  단점
		여러 속성이 동등하게 중요하고 독립적이라는 결함 가정 전제
		범주형 예측에는 적합하나 확률예측에는 정확도가 떨어짐
	  k개의 독립변수(설명변수)가 있다고 가정. 값에 대해서는 제약이 없음
	  한개의 종속변수가 있다고 가정. 0또는 1. 이분법적인 상황
	  독립변수 {Xi}를 선형조합하여 S변수(logit)을 만듦
	  종속변수 Y의 값이 1이 될 조건부 확률P(Y=1|{xi})은 "로지스틱 함수" 또는 "Sigmoid 함수"를 사용해서 계산됨
	  인공신경망에서 "활성화 함수(activation function)"의 역할을 함
	  혼돈행렬
		로지스틱회귀를 평가하는 평가표
		정확도만을 높이면 암환자 검사, 신용불량인 사람을 찾아내는 경우 리스크가 크기때문에 주의해야함
		Accuracy(정확도) = 정확하게 예측된 개수 / 전체 개수
		Sensitivity(민감도) = 정확하게 예측된 1의 개수 / 실제1의 개수
		Specificity(특이도) = 정확하게 예측된 0의 개수 / 실제0의 개수
		Precision(정밀도) = 정확하게 예측된 1의 개수 / 1로 예측된 계수
		Recall(재현율) = 민감도와 같은 의미
	트리, 랜덤포레스트, 에이다부스트
	
	@@@@@@@@@@@@@@@
	나이브 베이즈
		주어진 데이터로 학습 모형을 최적화하기 위해 귀납적 갱신이 필요
		귀납추론은 여러가지 결과를 바탕으로 원인을 밝혀냄
		데이터를 사용한 모델 갱신의 기본이 되는 개념
		종속변수 Y가 범주형인 경우에 적용. 데이터 사이즈가 크지 않아도 적용 가능
		장점
		개념과 이론이 단순하고 계산이 빠름
		노이즈, 결측치에 둔감함
		단순함에 비해 성능이 좋음
		고차원 데이터에도 적합
		단점
		여러 속성이 동등하게 중요하고 독립적이라는 결함 가정 전제
		독립변수가 수치형일 경우 정확성이 떨어짐
	@@@@@@@@@@@@@@@@
	KNN
	@@@@@@@@@@@@@@@@
	@@@@@@@@@@@@@@@@
	@@@@@@@@@@@@@@@@
	Support Vector Machine(SVM)
		성능 괜찮은 알고리즘 중 하나
		분류 정확성과 분류 마진을 최대로 하는 알고리즘
		커널(kernel): 마진이 최대화된 초평면 공간으로 좌표를 매핑. 다양한 커널이 있지만 주로 2->3처럼 한차원늘려 사용
		장점
			노이즈 데이터 영향을 크게 받지 않고 과적합 문제가 적음
		분류성능이 높음
		단점
			최적 분류를 찾기 위해서는 커널함수 및 매개변수에 대한 반복적인 테스트가 필요함
		해석이 어렵고 트레이닝에 시간이 많이 소요됨
		C_grid = [0.001, 0.01, 0.1, 1, 10]
		gamma_grid = [0.001, 0.01, 0.1, 1]
		parameters = {'C': C_grid, 'gamma' : gamma_grid}
		gridCV = GridSearchCV(SVC(kernel='rbf'), parameters, cv=10);
		gridCV.fit(X_train, Y_train)
		best_C = gridCV.best_params_['C']
		best_gamma = gridCV.best_params_['gamma']
		SVM_best = SVC(C=best_C,gamma=best_gamma)
		SVM_best.fit(X_train, Y_train);
		Y_pred = SVM_best.predict(X_test)
		print( "SVM best accuracy : " + str(np.round(metrics.accuracy_score(Y_test,Y_pred),3)))
############
	인공신경망
		장점
			선형분류가 불가능한 비선형 문제에도 탁월한 성능을 보임
		분류 및 수치예측에 모두 적용 가능
		단점
			모형의 해석이 어려움(은닉층 노드)
		연산에 비교적 많은 자원 필요
		과적합, 과소적합 문제가 쉽게 일어남
어떤 활성화 함수를 사용하는지에 따라 출력값이 달라지므로 적절한 '활성화 함수'를 사용하는 것이 중요함
  input data -> {활성화 함수} -> output data 어떤 값을 입력받고나서 특성한 수식을 사용해서 결과값 뱉어내는거라 보면됨
  활성화 함수 종류
     step function
	   가장 기본이 되는 활성화 함수이며 계단모양을 갖는다
	   어떤 임계값을 기준으로 활성화(1) 되거나 비활성화(0) 되는 형태
	 sigmoid function
	   0과 1사이의 값만 가질 수 있는 비선형 함수
	   step function은 0과 1만 가졌지만 sigmoid는 이 사이에 연속적인 출력값이 있을것이라 봄
	 ReLU function
	   sigmoid function의 Gradient Vanishing 문제를 해결하기위해 최근에 많이 사용되고 있는 활성화 함수
	     Gradient Vanishing 문제란 0과 1사이 값을 가지는 sigmoid function에서 아주 작은 값을 가지게 됨 ex)0.000001
		 Gradient Descent을 사용해서 back-propagation시 각 layer를 지나며 이를 지속적으로 곱해 줄 때
		 layer가 많으면 0으로 수렴하는 문제가 발생하여 sigmoid가 잘 작동하지 않는다
###########
	K Nearset Neighvor(KNN) 분류 알고리즘
		직관적 이해가 가능한 가장 간단한 알고리즘 중 하나
		주변의 가장 가까운 K개의 데이터 포인트를 찾아서 "다수결"로 값 결정
		K를 너무 적게 설정하면 노이즈에 민감해지고 overfitting 위험이 있음(분산 오류 증가)
		K를 너무 크게 설정하면 디테일에 둔감해지고 underfitting 위험이 있음(편향 오류 증가)
		장점
			간단하며 직관적인 이해 가능
		사전 모형 설정이 필요없고 모수에 대한 추정이 필요 없음(비모수 방법)
		단점
			예측은 효율적이지 못함
		모형이 없어 구조적 통찰력을 얻지 못함
		KNN 단독 적용
		knn = KNeighborsClassifier(n_neighbors=5, weights ='distance')
		knn.fit(X_train, Y_train)
		Y_pred = knn.predict(X_test)
		print( "Accuracy : " + str(np.round(metrics.accuracy_score(Y_test,Y_pred),3)))
		KNN + PCA 적용시
		pca = PCA(n_components = num) #n_components는 PCA 주성분 몇개까지 할껀지 
		X_pca = pca.fit_transform(X)
		X_train, X_test, Y_train, Y_test = train_test_split(X_pca,Y, test_size=0.3, random_state=3)
		knn = KNeighborsClassifier(n_neighbors=5, weights ='distance')
		knn.fit(X_train, Y_train);
		Y_pred = knn.predict(X_test)
		print( "Accuracy : " + str(np.round(metrics.accuracy_score(Y_test,Y_pred),3)))
	비지도학습
		군집분석: k-means, hierachical, DBSCAN
	주성분 분석(PCA), 비음수 행렬분해(NMF)
	t-SNE
	연관성 분석
	Topic Modeling
	LSA
	잠재 의미 분석 LSA (Latent Semantic Analysis)
		여러 문서를 분석하여 몇 개의 공통된 concept(개념)을 추출
			TF IDF 행렬로 SVD 분해를 하여 주요 성분을 추출
		TF IDF 행렬 A의 크기는 Size = 문서의개수m * 단어의개수n
		하나의 concept(토픽?) 벡터는 길이가 n 이며 행렬 A의 주성분임
		특이값 크기 순서로 2~5개의 concept를 추출하여 해석
	트리알고리즘
	과적합 현상이 쉽게 일어나므로 가지치기를 해야됨
	교차검증을 통한 가지치기	
	
자연어 분석 NLP (Natural Language Processing)
텍스트 문서에서 특징을 추출하여 분류, 요약, 군집화, 감성분석 등을 하는 분야
머신러닝과 언어학의 교차점에 있음
문서의 내용은 최소 분석 단위로 분해되고 불필요한 요소들은 여과됨(전처리)
문서의 내용을 '통계적'으로 분석함. 인간이 이해하는 방식과는 차이가 있음
비정형 데이터인 문서의 정형화 과정이 포함
	말뭉치: 언어적 자료의 집합을 의미
	작게는 소설한편, 크게는 SNS상의 수많은 텍스트자료들
	원시 말뭉치(raw corpus): 텍스트를 컴퓨터 가곡형 자료로 만들어서 DB에 저장해놓은것
	가공된 말뭉치(tagged corpus): 수집된 텍스트 데이터를 형태소 분석이나 어휘, 품사 정보, 문헌 내용 등등으로 인공적으로 가공한 말뭉치
	문서의 전처리
	문장 부호(마침표,쉼표)를 기준으로 문장을 분리. 영어인 경우에는 소문자화 포함
	분리된 문장에서 문장 부호, 특수 문자, 숫자 등 불필요 요소 제거
	분리된 문장에서 어절 단위로 분리. 어절은 띄어쓰기를 기준으로 구분
	불용어(stopword): 특별한 정보를 제공하지 못하는 단어. 반대는 가용어
	영어의 대표적 불용어는 관사와 전치사. ex) a, the, on, with
	한국어의 대표적 불용어는 조사와 어미. ex) 는, 을
	불용어는 불용어 사전을 사용하여 제거함
	키워드: 가용어 중에서 문서의 주체어
	키워드 선정은 분석하고자 하는 목적 및 문서의 특성을 따르는 것이 원칙
	하지만, 보통 문서 내에서 발생 빈도가 높은 단어들을 키워드로 선정
	형태소 분석: 형태소란 의미가 있는 최소 단위로서 더 이상 분리가 불가능한 요소
		주어진 단어 또는 어절의 형태소에서 기본형 및 품사 정보를 추출하는 것
		단어: 어절을 구성하는 단위로 어절은 하나 또는 두 개 이상의 단어로 구성. 나 + 는
		어절: 한국어에서는 띄어쓰기를 기준으로 어절을 구분. 나는 + 사과를 + 먹는다
	문법적, 관계적 뜻을 나타내는 단어 또는 단어의 부분
	어간과 어미 단위도 형태소로 간주
	어간을 분리해내는 텍스트 처리기술을 stemming 이라고 함
		intelligent, itelligence intelligently -> intelligen
		잡히고, 잡히다 -> 잡히
NLTK (Natural Language Tool Kit): 자연어 처리 및 문서 분석용 파이썬 패키지
한국어의 경우 추가적으로 KoNLPy설치가 필요한데 어려웡
#설치 명령어
	pip install nltk
	nltk.download()
주요기능
	품사 태깅(part of speech tagging): Penn Treebank Tagset에 의한 품사 태깅
	NN 명사, PRP 인칭대명사, VB 동사원형 등등.. 여기쓰긴 넘 많음
	한국어의 경우 21세기 세종계획 품사 태그세트 등이 있음
	말뭉치(corpus): 형태소, 품사 등을 구조적인 형태로 정리해 놓은 것
		별도 설치 필요: nltk.download('stopwords')
TF-IDF 모형
	BOW(Bag of Words): 여러 문서에 등장하는 단어들을 모아 놓은 집합체
	TF(Term Frequency): 여러 문서에 등장하는 단어들의 빈도수
	DF(Document Frequency): 특정 단어가 얼마나 많은 문서에 등장하는지에 대한 빈도수
	TDM or DTM(Term Document Matrix): 문서별로 사용된 단어의 빈도를 표시한 행렬
	IDF(Inverse Document Frequency): 단어의 희귀성을 나타냄. DF의 역수 또는 이 것의 로그
	TF-IDF: 특정 단어의 중요성을 나타냄. TF와 IDF의 곱
	similarity(유사도): TF-IDF 벡터 사이의 코사인 유사도. "내적"을 사용해서 구할 수 있따
	BOW(Bag of Words)
	각 단어는 문서의 의미를 나타내는 가장 기본적인 단위
	단어-문서 행렬(Bag of Words)는 단어간의 연관성을 고려하지 않은 추상화 모형
	문서간의 유사도 측정, 분류, 군집화, 요약 등의 목적으로 활용
	단어가 행을 이루고 문서가 열을 이루면 "단어-문서 행렬"
	단어가 열을 이루고 문서가 행을 이루면 "문서-단어 행렬"
	행렬의 셀은 0과 1의 binary 값을 갖는데 불포함 또는 포함을 의미함
	
	53
	
	LSA 선형대 수학적 해석
	LDA 베이즈 네트워크로 인한 확률적 해석