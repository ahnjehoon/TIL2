강화학습
에이전트는 순차적으로 의사결정을함. 즉, 시간의 개념이 중요한 역할을 함
보상은 즉각적이지 않고 몇스텝 이후에 주어짐
에이전트의 행동은 후속 수신 데이터에 영향을 미침
요소
	상태(state): 환경을 충분히 설명해 줄 수 있는 변수나 특징. 예를들면 센서정보들?
	환경(environment): 상태가 주어지면 전이가 가능한 구조의 주의 환경을 생각할 수 있음
	에이전트(agent): 환경과 상호작용하는 알고리즘 또는 시스템을 의미
	행동(action: 상태의 전이를 의미함. 에이전트가 직접 행동을 수행하거나 행동을 추천할 수 있음. 행동을 수행하면 주위환경으로 부터 보상 또는 벌점을 받음
	정책(policy): 정책은 특정 상태에서 에이전트가 취해야 할 행동을 정의함. 즉, 상태의 행동을 정의해 놓은것임. 행동은 확률적일수도 있고 확정적일수도 있음
에이전트 유형
	가치기반(value based)
		모든 상태는 어떤 값을 갖음(할인된 미래 보상의 합)
		출발지점의 상태에 해당하는 값은 비교적 낮음
		에이전트의 상태가 이 값을 키워가는 방향으로 전이가 진행됨
		도착지점의 상태의 값이 가장 큰 위치
		하지만 에이전트가 길을 잘못 들어서면 출발 지점 보다도 낮은 값의 상태에 빠질 수 있음(함정카드)
		이 유형에서는 정책이 명시되지 않는다
	정책기반(policy based)
		모든 상태에는 에이전트가 따라야 할 정책(방향)이 정의(매핑)되어 있음
		에이전트는 단순히 정책을 따르면 보상값이 최대화 되는 방향으로 최적화 됨
		이 유형에서는 상태에 따른 값이 없고 또한 가치 함수도 없음
	액터 크리틱(acto critic)
		정책과 가치함수가 함께 명시되어 있음.
		개별가치 함수 또는 정책보다는 뛰어남
마르코프 체인과 의사결정 과정
확률 과정
	시간이 지남에 따라서 조사된 확률변수의 집합
	상태가 확률적으로 변하고 이러한 정보를 모아놓은 집합
마르코프 과정(Markov process)
	미래의 움직임이 과거와는 관계없이, 현재의 상태에 따라서 정해지는 확률 과정
	마르코프 체인(Markov chain)은 시간이 이산적인 경우에 해당함
		이번 주에 식빵을 구매한 소비자는 다음주에도 95%의 확률로 식빵을 재구매함
		반면, 이번주에 식빵을 구매하지 않은 소비자는 20%의 확률로 다음주 식빵을 구매함
전이행렬(Transition matrix)
	전이행렬의 원소는 이전상태?s1에서 다음상태?s2로 전이되는 확률을 나타냄
	전이행렬의 행방향으로 합은 1이 되어야함
	안전상태(steady state): 전이행렬을 거듭해서 적용(곱하다보면)하다 보면 변화가 멈춘상태가 됨. 이것을 안정상태라 부름
마르코프 의사결정 과정(Markov devision process, MDP): 전제조건
	환경은 전체가 완전히 관찰 가능해야함
	현재상태가 과정의 특성을 모두 반영하여야 함. 미래의 상태는 현재의 상태에 의해서 결정되고 과거의 상태가 가치를 다시 살펴볼 필요는 없음
	강화학습의 문제는 조건부 확률 관계인 MDP로 구성할 수 있다.
	MDP는 조건부 확률에 대한 주장. 마르코프 과정을 따른다는 가정하 미래의 상태는 이전 상태에 대한 조건으로 만들어짐
	즉, 더 먼 과거로부터의 모든 필요한 정보가 반영된다는 것을 알 수 있음
	S = {s1, s2, s3 ...}에이전트가 취할 수 있는 상태(state)의 집합
	A = {a1, a2, a3 ...}에이전트가 취할수 있는 행동(action)의 집합임
벨만의 방정식(Bellman's Equation)
	벨만 방정식을 사용하여 마르코프 결정 과정(MDP)를 수학적으로 정의
	이 방정식을 통해서 주어진 환경에서의 최적 정책을 알아낼 수 있음
	"정책"이라 함은 t시점에서 에이전트의 상태가 s일때 a라는 취할 확률
	여기서 최적정책은 상태의 가치가 최대이
	
	노답~~~~~~~~~~~~~~~~~~~
	안해~~~~~~~~~~~~~~~
	