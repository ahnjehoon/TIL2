{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN(Generative Adversarial Network)\n",
    "- 위조지폐범(생성자)과 경찰(구분자)가 한쪽은 최대한 속이려고 한쪽은 최대한 감별하려고 노력하는것과 비슷함\n",
    "- 서로 대립(adversarial)하는 두 신경망을 경쟁시켜가며 결과물 생성 방법을 학습\n",
    "\n",
    "\n",
    "## 구조\n",
    "1. 실제 이미지를 주고 구분자(discriminator)에게 이 이미지가 진짜임을 판단하게 함\n",
    "2. 생성자(Generator)를 통해 노이즈로부터 임의의 이미지를 만들고 이것을 다시 같은 구분자를 통해 진짜 이미지인지 판단\n",
    "3. 이렇듯 생성자는 구분자를 속여 진짜처럼 보이게 하고, 구분자는 생성자가 만든 이미지를 최대한 가짜라고 구분하도록 훈련\n",
    "4. 경쟁을 하면서 결과적으로 생성자는 실제 이미지와 상당히 비슷한 이미지 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN 기본 모델 구현\n",
    "- 숫자 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-03052c1d1b02>:6: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Users\\PC\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Users\\PC\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./mnist/data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\PC\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./mnist/data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\PC\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting ./mnist/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\PC\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"./mnist/data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼 파라미터 설정\n",
    "# 마지막의 noise는 생성자의 입력값으로 사용할 노이즈의 크기\n",
    "total_epoch = 100\n",
    "batch_size = 100\n",
    "learning_rate = 0.0002\n",
    "n_hidden = 256\n",
    "n_input = 28 * 28\n",
    "n_noise = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\PC\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# 플레이스 홀더 설정\n",
    "# 비지도 학습이므로 Y를 사용하지 않음\n",
    "# 구분자에 넣을 이미지가 실제 이미지와 생성한 가짜 이미지 두 개이므로 Z변수 추가함\n",
    "X = tf.placeholder(tf.float32,[None, n_input])\n",
    "Z = tf.placeholder(tf.float32,[None, n_noise])\n",
    "\n",
    "# 생성자 신경망에 사용할 변수 설정\n",
    "# 첫 번째 가중치와 편향은 은닉층으로 출력하기 위한 변수들\n",
    "# 두 번째 가중치와 편향은 출력층에 사용할 변수들\n",
    "# 두번째 가중치의 변수 크기는 실제 이미지의 크기와 같아야함\n",
    "# 그 크기는 n_input으로 28 x 28인 784가 됨\n",
    "G_W1 = tf.Variable(tf.random_normal([n_noise, n_hidden], stddev = 0.01))\n",
    "G_b1 = tf.Variable(tf.zeros([n_hidden]))\n",
    "G_W2 = tf.Variable(tf.random_normal([n_hidden, n_input], stddev = 0.01))\n",
    "G_b2 = tf.Variable(tf.zeros([n_input]))\n",
    "\n",
    "# 구분자 신경망에 사용할 변수 설정\n",
    "# 은닉층은 생성자와 동일하게 구성\n",
    "# 구분자는 진짜와 얼마나 가까운가를 판단하는 값으로 0~1사이의 값을 출력\n",
    "D_W1 = tf.Variable(tf.random_normal([n_input, n_hidden], stddev=0.01))\n",
    "D_b1 = tf.Variable(tf.zeros([n_hidden]))\n",
    "D_W2 = tf.Variable(tf.random_normal([n_hidden, 1], stddev=0.01))\n",
    "D_b2 = tf.Variable(tf.zeros([1]))\n",
    "\n",
    "# 실제 이미지를 판별하는 구분자 신경망과 생성한 이미지를 판별하는 구분자 신경망은 같은 변수를 사용해야 함\n",
    "# 같은 신경망으로 구분을 시켜야 진짜 이미지와 가짜 이미지를 구분하는 특징들을 동시에 잡아낼 수 있기 때문"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 생성자와 구분자 신경망 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 생성자 신경망\n",
    "# 무작위로 생성한 노이즈를 받아 가중치와 편향을 반영하여 은닉층을 만들고\n",
    "# 은닉층에서 실제 이미지와 같은 크기의 결괏값을 출력하는 간단한 구성\n",
    "def generator(noise_z):\n",
    "    hidden = tf.nn.relu(tf.matmul(noise_z, G_W1) + G_b1)\n",
    "    output = tf.nn.sigmoid(tf.matmul(hidden, G_W2) + G_b2)\n",
    "    return output\n",
    "\n",
    "# 구분자 신경망\n",
    "# 0~1 사이의 스칼라값 하나를 출력\n",
    "def discriminator(inputs):\n",
    "    hidden = tf.nn.relu(tf.matmul(inputs, D_W1) + D_b1)\n",
    "    output = tf.nn.sigmoid(tf.matmul(hidden, D_W2) + D_b2)\n",
    "    return output\n",
    "\n",
    "# 무작위한 노이즈를 만들어주는 함수 정의\n",
    "def get_noise(batch_size, n_noise):\n",
    "    return np.random.normal(size=(batch_size, n_noise))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 노이즈 Z를 이용해 가짜이미지를 만들 생성자 G를 만들고\n",
    "# 이 G가 만든 가짜 이미지와 진짜이미지 X를 구분자에 넣어 입력한 이미지가 진짜인지 판별\n",
    "G = generator(Z)\n",
    "D_gene = discriminator(G)\n",
    "D_real = discriminator(X)\n",
    "\n",
    "# GAN 모델의 최적화는 loss_G 와 loss_D 를 최대화 하는 것\n",
    "# 진짜(D_real)는 1에 가까워 져야하고(진짜라고 판별)\n",
    "# 가짜(D_gene)는 0에 가까워야 함(가짜라고 판별)\n",
    "# D_real과 1에서 D_gene를 뺀 값을 더한 손실값으로 하여, 이 값을 최대화 하면 진짜판별기의 학습이 이뤄짐\n",
    "loss_D = tf.reduce_mean(tf.log(D_real) + tf.log(1 - D_gene))\n",
    "loss_G = tf.reduce_mean(tf.log(D_gene))\n",
    "\n",
    "# loss_D 를 구할 때는 판별기 신경망에 사용되는 변수만 사용하고,\n",
    "# loss_G 를 구할 때는 생성기 신경망에 사용되는 변수만 사용\n",
    "D_var_list = [D_W1, D_b1, D_W2, D_b2]\n",
    "G_var_list = [G_W1, G_b1, G_W2, G_b2]\n",
    "\n",
    "# GAN 논문의 수식에 따르면 loss 를 극대화 해야하지만, minimize 하는 최적화 함수를 사용하기 때문에\n",
    "# 최적화 하려는 loss_D 와 loss_G 에 음수 부호를 붙여줌\n",
    "train_D = tf.train.AdamOptimizer(learning_rate).minimize(-loss_D, var_list=D_var_list)\n",
    "train_G = tf.train.AdamOptimizer(learning_rate).minimize(-loss_G, var_list=G_var_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0000 D loss: -0.5376 G loss: -1.958\n",
      "Epoch: 0001 D loss: -0.4253 G loss: -2.095\n",
      "Epoch: 0002 D loss: -0.2533 G loss: -2.29\n",
      "Epoch: 0003 D loss: -0.4498 G loss: -1.508\n",
      "Epoch: 0004 D loss: -0.3957 G loss: -1.859\n",
      "Epoch: 0005 D loss: -0.3512 G loss: -2.364\n",
      "Epoch: 0006 D loss: -0.2423 G loss: -2.541\n",
      "Epoch: 0007 D loss: -0.249 G loss: -2.808\n",
      "Epoch: 0008 D loss: -0.1321 G loss: -3.141\n",
      "Epoch: 0009 D loss: -0.1639 G loss: -3.425\n",
      "Epoch: 0010 D loss: -0.277 G loss: -2.693\n",
      "Epoch: 0011 D loss: -0.3435 G loss: -2.662\n",
      "Epoch: 0012 D loss: -0.4828 G loss: -2.05\n",
      "Epoch: 0013 D loss: -0.5425 G loss: -2.432\n",
      "Epoch: 0014 D loss: -0.3296 G loss: -2.527\n",
      "Epoch: 0015 D loss: -0.4713 G loss: -1.996\n",
      "Epoch: 0016 D loss: -0.3661 G loss: -2.463\n",
      "Epoch: 0017 D loss: -0.4903 G loss: -2.506\n",
      "Epoch: 0018 D loss: -0.329 G loss: -2.647\n",
      "Epoch: 0019 D loss: -0.3301 G loss: -2.747\n",
      "Epoch: 0020 D loss: -0.4499 G loss: -2.633\n",
      "Epoch: 0021 D loss: -0.415 G loss: -2.592\n",
      "Epoch: 0022 D loss: -0.3673 G loss: -2.287\n",
      "Epoch: 0023 D loss: -0.4354 G loss: -2.306\n",
      "Epoch: 0024 D loss: -0.5204 G loss: -2.138\n",
      "Epoch: 0025 D loss: -0.5983 G loss: -2.184\n",
      "Epoch: 0026 D loss: -0.5275 G loss: -2.232\n",
      "Epoch: 0027 D loss: -0.6272 G loss: -1.989\n",
      "Epoch: 0028 D loss: -0.5564 G loss: -1.938\n",
      "Epoch: 0029 D loss: -0.5888 G loss: -2.445\n",
      "Epoch: 0030 D loss: -0.4272 G loss: -2.423\n",
      "Epoch: 0031 D loss: -0.7098 G loss: -1.829\n",
      "Epoch: 0032 D loss: -0.6116 G loss: -2.216\n",
      "Epoch: 0033 D loss: -0.6084 G loss: -1.888\n",
      "Epoch: 0034 D loss: -0.6705 G loss: -1.838\n",
      "Epoch: 0035 D loss: -0.6249 G loss: -1.831\n",
      "Epoch: 0036 D loss: -0.6872 G loss: -2.023\n",
      "Epoch: 0037 D loss: -0.5895 G loss: -2.236\n",
      "Epoch: 0038 D loss: -0.5654 G loss: -1.987\n",
      "Epoch: 0039 D loss: -0.7205 G loss: -1.986\n",
      "Epoch: 0040 D loss: -0.6591 G loss: -2.26\n",
      "Epoch: 0041 D loss: -0.6955 G loss: -2.026\n",
      "Epoch: 0042 D loss: -0.6474 G loss: -2.001\n",
      "Epoch: 0043 D loss: -0.847 G loss: -1.813\n",
      "Epoch: 0044 D loss: -0.8128 G loss: -1.628\n",
      "Epoch: 0045 D loss: -0.6723 G loss: -1.884\n",
      "Epoch: 0046 D loss: -0.8216 G loss: -2.009\n",
      "Epoch: 0047 D loss: -0.8106 G loss: -1.943\n",
      "Epoch: 0048 D loss: -0.6648 G loss: -2.022\n",
      "Epoch: 0049 D loss: -0.5992 G loss: -2.056\n",
      "Epoch: 0050 D loss: -0.7115 G loss: -2.02\n",
      "Epoch: 0051 D loss: -0.8549 G loss: -2.046\n",
      "Epoch: 0052 D loss: -0.7414 G loss: -2.038\n",
      "Epoch: 0053 D loss: -0.8366 G loss: -1.668\n",
      "Epoch: 0054 D loss: -0.91 G loss: -1.519\n",
      "Epoch: 0055 D loss: -0.659 G loss: -1.896\n",
      "Epoch: 0056 D loss: -0.896 G loss: -1.94\n",
      "Epoch: 0057 D loss: -0.717 G loss: -1.941\n",
      "Epoch: 0058 D loss: -0.7948 G loss: -1.879\n",
      "Epoch: 0059 D loss: -0.8518 G loss: -1.858\n",
      "Epoch: 0060 D loss: -0.8984 G loss: -1.709\n",
      "Epoch: 0061 D loss: -0.8426 G loss: -1.758\n",
      "Epoch: 0062 D loss: -0.8348 G loss: -1.84\n",
      "Epoch: 0063 D loss: -0.8222 G loss: -1.781\n",
      "Epoch: 0064 D loss: -0.8025 G loss: -2.09\n",
      "Epoch: 0065 D loss: -0.7583 G loss: -1.906\n",
      "Epoch: 0066 D loss: -0.7515 G loss: -1.796\n",
      "Epoch: 0067 D loss: -0.8173 G loss: -1.664\n",
      "Epoch: 0068 D loss: -0.7661 G loss: -1.783\n",
      "Epoch: 0069 D loss: -0.8746 G loss: -1.765\n",
      "Epoch: 0070 D loss: -1.002 G loss: -1.794\n",
      "Epoch: 0071 D loss: -0.8789 G loss: -1.571\n",
      "Epoch: 0072 D loss: -0.8372 G loss: -1.819\n",
      "Epoch: 0073 D loss: -0.7753 G loss: -1.696\n",
      "Epoch: 0074 D loss: -1.062 G loss: -1.573\n",
      "Epoch: 0075 D loss: -0.8432 G loss: -1.399\n",
      "Epoch: 0076 D loss: -0.89 G loss: -1.607\n",
      "Epoch: 0077 D loss: -0.8697 G loss: -1.68\n",
      "Epoch: 0078 D loss: -1.067 G loss: -1.654\n",
      "Epoch: 0079 D loss: -0.8571 G loss: -1.599\n",
      "Epoch: 0080 D loss: -0.9238 G loss: -1.456\n",
      "Epoch: 0081 D loss: -0.9074 G loss: -1.625\n",
      "Epoch: 0082 D loss: -0.8047 G loss: -1.696\n",
      "Epoch: 0083 D loss: -0.9426 G loss: -1.665\n",
      "Epoch: 0084 D loss: -0.8334 G loss: -1.89\n",
      "Epoch: 0085 D loss: -0.8506 G loss: -1.58\n",
      "Epoch: 0086 D loss: -0.9279 G loss: -1.607\n",
      "Epoch: 0087 D loss: -0.7928 G loss: -1.825\n",
      "Epoch: 0088 D loss: -0.9278 G loss: -1.562\n",
      "Epoch: 0089 D loss: -0.9412 G loss: -1.592\n",
      "Epoch: 0090 D loss: -0.9219 G loss: -1.797\n",
      "Epoch: 0091 D loss: -0.8182 G loss: -1.694\n",
      "Epoch: 0092 D loss: -0.7095 G loss: -1.659\n",
      "Epoch: 0093 D loss: -0.8594 G loss: -1.651\n",
      "Epoch: 0094 D loss: -0.8798 G loss: -1.74\n",
      "Epoch: 0095 D loss: -0.9253 G loss: -1.649\n",
      "Epoch: 0096 D loss: -0.82 G loss: -1.641\n",
      "Epoch: 0097 D loss: -0.8608 G loss: -1.678\n",
      "Epoch: 0098 D loss: -0.8771 G loss: -1.564\n",
      "Epoch: 0099 D loss: -0.7578 G loss: -1.855\n",
      "최적화 완료!\n"
     ]
    }
   ],
   "source": [
    "# 신경망 모델 학습\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "total_batch = int(mnist.train.num_examples/batch_size)\n",
    "loss_val_D, loss_val_G = 0, 0\n",
    "\n",
    "for epoch in range(total_epoch):\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        noise = get_noise(batch_size, n_noise)\n",
    "\n",
    "        # 판별기와 생성기 신경망을 각각 학습\n",
    "        _, loss_val_D = sess.run([train_D, loss_D],\n",
    "                                 feed_dict={X: batch_xs, Z: noise})\n",
    "        _, loss_val_G = sess.run([train_G, loss_G],\n",
    "                                 feed_dict={Z: noise})\n",
    "\n",
    "    print('Epoch:', '%04d' % epoch,\n",
    "          'D loss: {:.4}'.format(loss_val_D),\n",
    "          'G loss: {:.4}'.format(loss_val_G))\n",
    "\n",
    "    # 학습이 되어가는 모습을 보기 위해 주기적으로 이미지를 생성하여 저장\n",
    "    if epoch == 0 or (epoch + 1) % 10 == 0:\n",
    "        sample_size = 10\n",
    "        noise = get_noise(sample_size, n_noise)\n",
    "        samples = sess.run(G, feed_dict={Z: noise})\n",
    "\n",
    "        fig, ax = plt.subplots(1, sample_size, figsize=(sample_size, 1))\n",
    "\n",
    "        for i in range(sample_size):\n",
    "            ax[i].set_axis_off()\n",
    "            ax[i].imshow(np.reshape(samples[i], (28, 28)))\n",
    "\n",
    "        plt.savefig('samples/{}.png'.format(str(epoch).zfill(3)), bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "        \n",
    "print('최적화 완료!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 원하는 숫자 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_epoch = 100\n",
    "batch_size = 100\n",
    "learning_rate = 0.0002\n",
    "n_hidden = 256\n",
    "n_input = 28 * 28\n",
    "n_noise = 128\n",
    "n_class = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, [None, n_input])\n",
    "# 노이즈와 실제 이미지에, 그에 해당하는 숫자에 대한 정보를 넣어주기 위해 사용\n",
    "Y = tf.placeholder(tf.float32, [None, n_class])\n",
    "Z = tf.placeholder(tf.float32, [None, n_noise])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(noise, labels):\n",
    "    with tf.variable_scope('generator'):\n",
    "        # noise 값에 labels 정보를 추가\n",
    "        inputs = tf.concat([noise, labels], 1)\n",
    "        # tf.layers.dense 함수를 이용해 은닉층(hidden) 작성\n",
    "        hidden = tf.layers.dense(inputs, n_hidden, activation=tf.nn.relu)\n",
    "        # 진짜 이미지와 같은 크기의 값을 만드는 출력층 작성\n",
    "        output = tf.layers.dense(hidden, n_input, activation=tf.nn.sigmoid)\n",
    "\n",
    "    return output\n",
    "\n",
    "# 구분자 신경망 정의\n",
    "def discriminator(inputs, labels, reuse=None):\n",
    "    with tf.variable_scope('discriminator') as scope:\n",
    "        # 노이즈에서 생성한 이미지와 실제 이미지를 판별하는 모델의 변수를 동일하게 하기 위해,\n",
    "        # 이전에 사용되었던 변수를 재사용\n",
    "        if reuse:\n",
    "            scope.reuse_variables()\n",
    "\n",
    "        inputs = tf.concat([inputs, labels], 1)\n",
    "        hidden = tf.layers.dense(inputs, n_hidden, activation=tf.nn.relu)\n",
    "        output = tf.layers.dense(hidden, 1, activation=None)\n",
    "\n",
    "    return output\n",
    "\n",
    "def get_noise(batch_size, n_noise):\n",
    "    return np.random.uniform(-1., 1., size=[batch_size, n_noise])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-10-aec4854152d5>:6: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n"
     ]
    }
   ],
   "source": [
    "# 생성자 구성\n",
    "# 생성자에는 레이블 정보를 추가하여 추후 레이블 정보에 해당하는 이미지를 생성할 수 있도록 유도\n",
    "G = generator(Z, Y)\n",
    "D_real = discriminator(X, Y)\n",
    "D_gene = discriminator(G, Y, True)\n",
    "\n",
    "# D_real 값은 1에, D_gene값은 0에 가까워 지게 하는 것인데\n",
    "# tf.nn.sigmoid_cross_entropy_with_logits 함수를 이요하면 보다 간편하게 작성가능\n",
    "loss_D_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_real, labels=tf.ones_like(D_real)))\n",
    "loss_D_gene = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_gene, labels=tf.zeros_like(D_gene)))\n",
    "# loss_D_real 과 loss_D_gene 을 더한 뒤 이 값을 최소화하면 구분자를 학습시킬 수 있음\n",
    "loss_D = loss_D_real + loss_D_gene\n",
    "# 가짜 이미지를 진짜에 가깝게 만들도록 생성망을 학습시키기 위해, D_gene 을 최대한 1에 가깝도록 만드는 손실함수\n",
    "loss_G = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_gene, labels=tf.ones_like(D_gene)))\n",
    "\n",
    "# TensorFlow 에서 제공하는 유틸리티 함수를 이용해\n",
    "# discriminator 와 generator scope 에서 사용된 변수들을 쉽게 가져올 수 있음\n",
    "vars_D = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='discriminator')\n",
    "vars_G = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='generator')\n",
    "\n",
    "train_D = tf.train.AdamOptimizer().minimize(loss_D, var_list=vars_D)\n",
    "train_G = tf.train.AdamOptimizer().minimize(loss_G, var_list=vars_G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0000 D loss: 0.003557 G loss: 7.655\n",
      "Epoch: 0001 D loss: 0.02505 G loss: 6.854\n",
      "Epoch: 0002 D loss: 0.01085 G loss: 8.782\n",
      "Epoch: 0003 D loss: 0.01865 G loss: 8.482\n",
      "Epoch: 0004 D loss: 0.04829 G loss: 7.966\n",
      "Epoch: 0005 D loss: 0.02063 G loss: 7.595\n",
      "Epoch: 0006 D loss: 0.006697 G loss: 8.0\n",
      "Epoch: 0007 D loss: 0.1096 G loss: 7.643\n",
      "Epoch: 0008 D loss: 0.1533 G loss: 6.908\n",
      "Epoch: 0009 D loss: 0.114 G loss: 7.381\n",
      "Epoch: 0010 D loss: 0.1715 G loss: 5.471\n",
      "Epoch: 0011 D loss: 0.2219 G loss: 4.871\n",
      "Epoch: 0012 D loss: 0.3874 G loss: 4.96\n",
      "Epoch: 0013 D loss: 0.2477 G loss: 4.651\n",
      "Epoch: 0014 D loss: 0.3568 G loss: 4.587\n",
      "Epoch: 0015 D loss: 0.4408 G loss: 3.941\n",
      "Epoch: 0016 D loss: 0.4865 G loss: 3.585\n",
      "Epoch: 0017 D loss: 0.292 G loss: 3.82\n",
      "Epoch: 0018 D loss: 0.3567 G loss: 4.433\n",
      "Epoch: 0019 D loss: 0.5217 G loss: 3.196\n",
      "Epoch: 0020 D loss: 0.7289 G loss: 2.802\n",
      "Epoch: 0021 D loss: 0.8913 G loss: 2.158\n",
      "Epoch: 0022 D loss: 0.5318 G loss: 3.297\n",
      "Epoch: 0023 D loss: 0.6443 G loss: 2.216\n",
      "Epoch: 0024 D loss: 0.3998 G loss: 3.184\n",
      "Epoch: 0025 D loss: 0.5913 G loss: 2.824\n",
      "Epoch: 0026 D loss: 0.5798 G loss: 2.718\n",
      "Epoch: 0027 D loss: 0.6833 G loss: 2.656\n",
      "Epoch: 0028 D loss: 0.7008 G loss: 2.184\n",
      "Epoch: 0029 D loss: 0.6755 G loss: 2.519\n",
      "Epoch: 0030 D loss: 0.8496 G loss: 2.182\n",
      "Epoch: 0031 D loss: 0.8491 G loss: 2.394\n",
      "Epoch: 0032 D loss: 0.6617 G loss: 2.322\n",
      "Epoch: 0033 D loss: 0.7667 G loss: 2.081\n",
      "Epoch: 0034 D loss: 0.6792 G loss: 2.209\n",
      "Epoch: 0035 D loss: 0.6813 G loss: 2.316\n",
      "Epoch: 0036 D loss: 0.6751 G loss: 2.116\n",
      "Epoch: 0037 D loss: 0.7061 G loss: 2.014\n",
      "Epoch: 0038 D loss: 0.7766 G loss: 2.25\n",
      "Epoch: 0039 D loss: 0.7793 G loss: 1.976\n",
      "Epoch: 0040 D loss: 0.621 G loss: 1.98\n",
      "Epoch: 0041 D loss: 0.9433 G loss: 1.97\n",
      "Epoch: 0042 D loss: 0.7798 G loss: 2.234\n",
      "Epoch: 0043 D loss: 0.6276 G loss: 2.251\n",
      "Epoch: 0044 D loss: 0.5691 G loss: 2.067\n",
      "Epoch: 0045 D loss: 0.788 G loss: 1.85\n",
      "Epoch: 0046 D loss: 0.6854 G loss: 2.106\n",
      "Epoch: 0047 D loss: 0.6213 G loss: 2.513\n",
      "Epoch: 0048 D loss: 0.6364 G loss: 1.987\n",
      "Epoch: 0049 D loss: 0.7563 G loss: 2.258\n",
      "Epoch: 0050 D loss: 0.6942 G loss: 2.184\n",
      "Epoch: 0051 D loss: 0.6659 G loss: 2.133\n",
      "Epoch: 0052 D loss: 0.7624 G loss: 2.036\n",
      "Epoch: 0053 D loss: 0.7924 G loss: 2.063\n",
      "Epoch: 0054 D loss: 0.5817 G loss: 2.311\n",
      "Epoch: 0055 D loss: 0.5983 G loss: 2.145\n",
      "Epoch: 0056 D loss: 0.681 G loss: 2.844\n",
      "Epoch: 0057 D loss: 0.611 G loss: 2.542\n",
      "Epoch: 0058 D loss: 0.6867 G loss: 2.314\n",
      "Epoch: 0059 D loss: 0.666 G loss: 2.095\n",
      "Epoch: 0060 D loss: 0.6024 G loss: 2.357\n",
      "Epoch: 0061 D loss: 0.6828 G loss: 2.016\n",
      "Epoch: 0062 D loss: 0.684 G loss: 2.321\n",
      "Epoch: 0063 D loss: 0.79 G loss: 2.143\n",
      "Epoch: 0064 D loss: 0.748 G loss: 1.964\n",
      "Epoch: 0065 D loss: 0.6972 G loss: 1.874\n",
      "Epoch: 0066 D loss: 0.7251 G loss: 2.328\n",
      "Epoch: 0067 D loss: 0.869 G loss: 1.921\n",
      "Epoch: 0068 D loss: 0.6854 G loss: 2.166\n",
      "Epoch: 0069 D loss: 0.7398 G loss: 1.925\n",
      "Epoch: 0070 D loss: 0.7627 G loss: 1.971\n",
      "Epoch: 0071 D loss: 0.7047 G loss: 2.177\n",
      "Epoch: 0072 D loss: 0.7285 G loss: 2.193\n",
      "Epoch: 0073 D loss: 0.6486 G loss: 2.2\n",
      "Epoch: 0074 D loss: 0.7811 G loss: 2.068\n",
      "Epoch: 0075 D loss: 0.7058 G loss: 2.08\n",
      "Epoch: 0076 D loss: 0.6756 G loss: 2.071\n",
      "Epoch: 0077 D loss: 1.019 G loss: 1.953\n",
      "Epoch: 0078 D loss: 0.6688 G loss: 2.12\n",
      "Epoch: 0079 D loss: 0.9281 G loss: 1.953\n",
      "Epoch: 0080 D loss: 0.7421 G loss: 2.147\n",
      "Epoch: 0081 D loss: 0.7006 G loss: 2.164\n",
      "Epoch: 0082 D loss: 0.6555 G loss: 2.126\n",
      "Epoch: 0083 D loss: 0.6791 G loss: 2.278\n",
      "Epoch: 0084 D loss: 0.887 G loss: 1.778\n",
      "Epoch: 0085 D loss: 0.603 G loss: 2.232\n",
      "Epoch: 0086 D loss: 0.6545 G loss: 2.069\n",
      "Epoch: 0087 D loss: 0.6495 G loss: 2.466\n",
      "Epoch: 0088 D loss: 0.7254 G loss: 2.167\n",
      "Epoch: 0089 D loss: 0.6521 G loss: 2.507\n",
      "Epoch: 0090 D loss: 0.5859 G loss: 2.038\n",
      "Epoch: 0091 D loss: 0.731 G loss: 2.587\n",
      "Epoch: 0092 D loss: 0.672 G loss: 2.148\n",
      "Epoch: 0093 D loss: 0.5993 G loss: 2.132\n",
      "Epoch: 0094 D loss: 0.5806 G loss: 2.176\n",
      "Epoch: 0095 D loss: 0.7881 G loss: 1.899\n",
      "Epoch: 0096 D loss: 0.6326 G loss: 2.298\n",
      "Epoch: 0097 D loss: 0.7057 G loss: 1.978\n",
      "Epoch: 0098 D loss: 0.6919 G loss: 2.327\n",
      "Epoch: 0099 D loss: 0.6672 G loss: 2.181\n",
      "최적화 완료!\n"
     ]
    }
   ],
   "source": [
    "# 신경망 모델 학습\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "total_batch = int(mnist.train.num_examples/batch_size)\n",
    "loss_val_D, loss_val_G = 0, 0\n",
    "\n",
    "for epoch in range(total_epoch):\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        noise = get_noise(batch_size, n_noise)\n",
    "\n",
    "        _, loss_val_D = sess.run([train_D, loss_D],\n",
    "                                 feed_dict={X: batch_xs, Y: batch_ys, Z: noise})\n",
    "        _, loss_val_G = sess.run([train_G, loss_G],\n",
    "                                 feed_dict={Y: batch_ys, Z: noise})\n",
    "\n",
    "    print('Epoch:', '%04d' % epoch,\n",
    "          'D loss: {:.4}'.format(loss_val_D),\n",
    "          'G loss: {:.4}'.format(loss_val_G))\n",
    "\n",
    "    # 학습이 되어가는 모습을 보기 위해 주기적으로 레이블에 따른 이미지를 생성하여 저장\n",
    "    if epoch == 0 or (epoch + 1) % 10 == 0:\n",
    "        sample_size = 10\n",
    "        noise = get_noise(sample_size, n_noise)\n",
    "        samples = sess.run(G,\n",
    "                           feed_dict={Y: mnist.test.labels[:sample_size],\n",
    "                                      Z: noise})\n",
    "\n",
    "        fig, ax = plt.subplots(2, sample_size, figsize=(sample_size, 2))\n",
    "\n",
    "        for i in range(sample_size):\n",
    "            ax[0][i].set_axis_off()\n",
    "            ax[1][i].set_axis_off()\n",
    "\n",
    "            ax[0][i].imshow(np.reshape(mnist.test.images[i], (28, 28)))\n",
    "            ax[1][i].imshow(np.reshape(samples[i], (28, 28)))\n",
    "\n",
    "        plt.savefig('samples2/{}.png'.format(str(epoch).zfill(3)), bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "\n",
    "print('최적화 완료!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 초짜 대학원생 입장에서 이해하는 GAN\n",
    "  https://goo.gl/ZvSvtm 참조"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
