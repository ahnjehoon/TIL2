{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 일반적인 프로그래밍과는 약간 다른 개념을 가지고 있음\n",
    "    - 텐서(tensor)\n",
    "    - 플레이스홀더(placeholder)\n",
    "    - 변수(variable)\n",
    "    - 연산\n",
    "- 언어속의 또 다른 언어같은 느낌.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 텐서와 그래프 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const:0\", shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "hello = tf.constant('Hello, TensorFlow!')\n",
    "print(hello)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## constant\n",
    "tf.constant로 상수를 선언후 출력하면  \n",
    "```Tensor(\"Const:0\", shape=(), dtype=string)```  \n",
    "처럼 나오는데 **Tensor**형이고 string타입의 데이터라는 것을 알려줌  \n",
    "constant는 일반적인 변수 선언과 같음  \n",
    "tensorflow는 또 다른 언어같은 느낌이니 여기서 hello라는 python변수에 tensorflow변수를 박아뒀다라고 봐야될지..?  \n",
    "\n",
    "- 텐서는 텐서플로에서 다양한 수학식을 계산하기 위한 가장 기본적인 자료형\n",
    "- **rank**와 **shape**라는 개념을 가지고 있음\n",
    "\n",
    "    - 3 - rank가 0인 텐서. shape는 []\n",
    "    - [1,2,3] - rank가 1인 텐서. shape는 [3]\n",
    "    - [[1,2,3],[4,5,6]] - rank가 2인 텐서. shape는 [2,3]\n",
    "    - [[[1,2,3]],[[7,8,9]]] - rank가 3인 텐서. shape는 [2,1,3]\n",
    "\n",
    "- rank는 차원의 수. shape는 요소의 갯수를 뜻함\n",
    "- rank가 0이면 스칼라, 1이면 벡터, 2면 행렬, 3이면 n-Tensor 또는 n차원 텐서라함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Add:0\", shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant(10)\n",
    "b = tf.constant(32)\n",
    "c = tf.add(a, b)  # a + b 로도 쓸 수 있음\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 그래프\n",
    "출력시 42가 아닌\n",
    "```Tensor(\"Add:0\", shape=(), dtype=int32)``` 이 출력되는 이유는  \n",
    "텐서플로 프로그램의 구조가 두가지로 분리되어 있기 때문\n",
    "1. 그래프 생성\n",
    "```\n",
    "    a = tf.constant(10)\n",
    "    b = tf.constant(32)\n",
    "    c = tf.add(a, b)\n",
    "```\n",
    "2. 그래프 실행(아래 코드)\n",
    "```\n",
    "    print(sess.run(hello))\n",
    "    print(sess.run([a, b, c]))\n",
    "```\n",
    "\n",
    "그래프는 텐서들의 연산 모음이며  \n",
    "필요할 때 연산을 실행하는 코드를 넣어 '**원하는 시점**'에  실제 연산을 수행함  \n",
    "이를 **지연 실행(lazy evaluation)**이라고 하며 함수형 프로그래밍에서 많이 사용  \n",
    "실제 계산은 C++로 구현한 코어라이브러리에서 수행. 파이썬에서 동작하지만 빠른이유"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Hello, TensorFlow!'\n",
      "[10, 32, 42]\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "print(sess.run(hello))\n",
    "print(sess.run([a, b, c]))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Hello, TensorFlow!'\n",
      "[10, 32, 42]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    print(sess.run(hello))\n",
    "    print(sess.run([a, b, c]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그래프의 실행은 Session 안에서 이뤄져야하고 Session 객체와 run을 이용하면 됨  \n",
    "세션을 열고 닫는게 싫다면 위와 같은 코드로  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 플레이스홀더와 변수\n",
    "\n",
    "아직까지도 혼란스러운 개념임..  \n",
    "- 플레이스홀더 - 그래프에 사용할 입력값을 나중에 받기 위해 사용하는 매개변수(parameter)와 비슷함\n",
    "- 변수 - ㄹㅇ 변하는수. 프로그래밍에서 매개변수의 그 변수가 아닌 적절한 값을 찾아 학습시마다 계속 변하는 수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Placeholder:0\", shape=(?, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "X = tf.placeholder(tf.float32, [None, 3])\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Tensor(\"Placeholder:0\", shape=(?, 3), dtype=float32)```  \n",
    "Placeholder형이며 (?,3)모양의 float32형태의 데이터가 생성됨  \n",
    "여기서 (?,3)이란 [None, 3]으로 설정해서 열은 3으로 정해졌지만 받아들일 행의 크기가 정해지지 않았다는 뜻  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [[1, 2, 3], [4, 5, 6]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X 플레이스홀더에 넣을 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\PC\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "W = tf.Variable(tf.random_normal([3, 2]))\n",
    "b = tf.Variable(tf.random_normal([2, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.Variable: 그래프를 계산하면서 최적화 할 변수  \n",
    "tf.random_normal: 각 변수들의 초기값을 정규분포 랜덤 값으로 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "expr = tf.matmul(X, W) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "입력값과 변수들을 계산할 수식을 작성  \n",
    "tf.matmul 처럼 mat* 로 되어 있는 함수로 행렬 계산을 수행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 행렬곱\n",
    "- 행렬곱 R1C1 x R2C2(R = Row 행, C = Column 열)에 대해 C1의 갯수는 = R2의 갯수 여야함\n",
    "- 결과는 R1의 갯수 x C2의 갯수로 나옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== x_data ===\n",
      "[[1, 2, 3], [4, 5, 6]]\n",
      "=== W ===\n",
      "[[ 1.8680998   0.29095984]\n",
      " [-0.23848693 -0.20896561]\n",
      " [ 0.95428354  1.1176282 ]]\n",
      "=== b ===\n",
      "[[ 2.161185  ]\n",
      " [-0.18224257]]\n",
      "=== expr ===\n",
      "[[ 6.415162   5.3870983]\n",
      " [11.823423   6.6425385]]\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "# Variable 들의 값들을 초기화 하기 위해 tf.global_variables_initializer 를 한 번 실행해야 함.\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "print(\"=== x_data ===\")\n",
    "print(x_data)\n",
    "print(\"=== W ===\")\n",
    "print(sess.run(W))\n",
    "print(\"=== b ===\")\n",
    "print(sess.run(b))\n",
    "print(\"=== expr ===\")\n",
    "# feed_dict는 그래프를 실행할 때 사용할 입력값을 지정하는 것임\n",
    "# 여기서는 X x W + b의 X에 x_data박는다는뜻\n",
    "print(sess.run(expr, feed_dict={X: x_data}))\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 선형회귀\n",
    "\n",
    "- 주어진 x와 y값을 가지고 서로 간의 관계를 파악하는 것\n",
    "- 입력에 대해 출력을 예측하는것. 머신러닝의 기본"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x_data = [1,2,3]\n",
    "y_data = [1,2,3]\n",
    "\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "\n",
    "# random_uniform - 균등분포. 여기서는 -1 ~ 1사이의 값을 갖는 무작위값으로 초기화\n",
    "W = tf.Variable(tf.random_uniform([1], -1.0, 1.0))\n",
    "b = tf.Variable(tf.random_uniform([1], -1.0, 1.0))\n",
    "\n",
    "# W는 가중치 weight\n",
    "# b는 편향 bias라고도 함\n",
    "#hypothesis = tf.matmul(X, W) + b\n",
    "hypothesis = X * W + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 손실함수(loss function)\n",
    "한 쌍(x, y)의 데이터에 대한 **손실값**을 계산하는 함수  \n",
    "실제값과 예측값이 얼마나 차이가 있는지 나타내는 값을 뜻함  \n",
    "\n",
    "손실을 전체 데이터에 대해 구한 경우 이를 **비용**이라고 함  \n",
    "**학습**이란 변수값을 다양하게 넣어 손실값을 최소화 하는 W와 b의 값을 구하는 것  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\PC\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "# 손실 함수를 작성\n",
    "# mean(h - Y)^2 : 예측값과 실제값의 거리를 비용(손실) 함수로 정함\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "# 텐서플로우에 기본적으로 포함되어 있는 함수를 이용해 경사 하강법 최적화를 수행\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.05)\n",
    "# 비용을 최소화 하는 것이 최종 목표\n",
    "train_op = optimizer.minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 10.1233635 [0.23794448] [0.24063179]\n",
      "50 0.010210205 [0.88405246] [0.26357594]\n",
      "100 0.003046954 [0.93666023] [0.14398627]\n",
      "150 0.0009092707 [0.9653988] [0.07865676]\n",
      "200 0.00027134715 [0.98109806] [0.0429686]\n",
      "250 8.0976686e-05 [0.98967427] [0.02347287]\n",
      "\n",
      "=== Test ===\n",
      "X: 5, Y: [4.9844317]\n",
      "X: 2.5, Y: [2.4987051]\n"
     ]
    }
   ],
   "source": [
    "### 수정중\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # 최적화를 100번 수행\n",
    "    for step in range(300):\n",
    "        # sess.run 을 통해 train_op 와 cost 그래프를 계산\n",
    "        # 이 때, 가설 수식에 넣어야 할 실제값을 feed_dict 을 통해 전달\n",
    "        _, cost_val = sess.run([train_op, cost], feed_dict={X: x_data, Y: y_data})\n",
    "        \n",
    "        if step % 50 == 0: print(step, cost_val, sess.run(W), sess.run(b))\n",
    "\n",
    "    print(\"\\n=== Test ===\")\n",
    "    print(\"X: 5, Y:\", sess.run(hypothesis, feed_dict={X: 5}))\n",
    "    print(\"X: 2.5, Y:\", sess.run(hypothesis, feed_dict={X: 2.5}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
