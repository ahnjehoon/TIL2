{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 크롤링 전략"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iframe 태그의 설정 되어 있는 다른 웹 페이지의 주소를 파악하여 해당 페이지로 들어가 본다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 원하는 데이터가 어떤 페이지에 있는가...\n",
    "\n",
    "### 2. 우클릭 \n",
    "* 페이지 소스 보기만 있는 경우.. => 4번으로 이동\n",
    "* 프레임 소스 보기도 있는 경우.. => 3번으로 이동\n",
    "\n",
    "### 3. iframe 태그의 설정되어 있는 다른 웹 페이지의 주소를 파악하여 해당 페이지로 들어가본다.\n",
    "\n",
    "### 4. 가져올 데이터중에 제일 위에 있는 문자열을 페이지 소스에서 찾는다.\n",
    "* 있다 => bs4를 통해 분석    \n",
    "* 없다\n",
    "       -> Ajax인가... -> 주소 파악이 가능한가 -> bs4\n",
    "                      -> 주소 파악이 불가한가 -> selenium\n",
    "       -> web socket인가 -> selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "간단 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as rq\n",
    "import bs4\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 크롤링 할 사이트 주소\n",
    "url = 'https://pjt3591oo.github.io/'\n",
    "\n",
    "# 페이지를 요청하여 응답 결과를 가져옴\n",
    "response = rq.get(url)\n",
    "# print(response.text)\n",
    "\n",
    "# 응답 결과를 통해 request 객체를 만듦\n",
    "soup = bs4.BeautifulSoup(response.text, 'lxml')\n",
    "# print(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "react를 시작하기 전에 환경셋팅을 해보자\n",
      "query의 외부 라이브러리가 아닌 drag, drop 이벤트를 활용하여 기능 구현해보기\n",
      "mysqldump를 이용하여 디비 백업과 source를 이용하여 데이터 복원을 해보자\n",
      "mysql 디비설정, 유저설정을 통해 원격접속\n",
      "docx, hwp, pdf 파일\n",
      "pdf파일 html로 변경하기\n",
      "remote set-url을 이용하여 원격 저장소를 바꾸자\n",
      "working directory설정을 하여 경로 문제를 해결하자\n",
      "도커의 명령어들을 간단하게 알아보기\n",
      "python에서 utc를 timestamp로 바꾸는 방법\n"
     ]
    }
   ],
   "source": [
    "#[a.text.strip() for a in soup.select('body > main > div > div > div > h4')]\n",
    "\n",
    "# soup.select('div h4')\n",
    "for a in soup.select('body > main > div > div > div > h4'):\n",
    "    print(a.text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-05-20 06:29:05 +0000\n",
      "박정태\n"
     ]
    }
   ],
   "source": [
    "temp_str = soup.select('p.author > span.date')[0].text.strip()\n",
    "# print(temp_str)\n",
    "date_str = [x.strip() for x in temp_str.split('|')]\n",
    "print(date_str[0])\n",
    "print(date_str[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# @@@@@@@@"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주소를 받아 요청하고 class가 p인 div 태그 리스트를 반환한다\n",
    "def getDivList(url) : \n",
    "    # 요청\n",
    "    response = rq.get(url)\n",
    "    \n",
    "    # 응답 결과를 가져와 태그 객체를 만든다\n",
    "    soup = bs4.BeautifulSoup(response.text)\n",
    "    # class가 p인 div 태그 객체 리스트를 출력한다\n",
    "    div_list = soup.find_all('div', class_='p')\n",
    "    # 다음 페이지가 있는지 확인\n",
    "    # Next 문자열 값을 가지고 있는 태그를 가져옴\n",
    "    next_tag = soup.find(any, text='Next')\n",
    "    next_target = None\n",
    "    if next_tag.name == 'a': \n",
    "        next_target = next_tag.attrs['href']\n",
    "    \n",
    "    # 반환한다\n",
    "    return div_list, next_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData(div_tag) :\n",
    "    h3_tag = div_tag.find('h3')\n",
    "    a_tag = h3_tag.find('a')\n",
    "    href = a_tag.attrs['href']\n",
    "    a_text = a_tag.text.strip()\n",
    "    \n",
    "    # h4\n",
    "    h4_tag = div_tag.find('h4')\n",
    "    h4_text = h4_tag.text.strip()\n",
    "    \n",
    "    # date, author\n",
    "    p_tag = div_tag.find('p', class_='author')\n",
    "    span_tag = p_tag.find('span', class_='date')\n",
    "    temp_str = span_tag.text.strip()\n",
    "    temp_list = temp_str.split('|')\n",
    "    date_str = temp_list[0].strip()\n",
    "    author_str = temp_list[1].strip()\n",
    "    \n",
    "    # 하나의 튜플로 반환\n",
    "    return href, a_text, h4_text, date_str, author_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 리스트에 담긴 데이터를 출력한다\n",
    "def printList(data_list) : \n",
    "    print('-----------------------------')\n",
    "    for x in data_list:\n",
    "        for key, value in x.items():\n",
    "            print(f'{key} : {value}')\n",
    "        print('-----------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file C:\\ProgramData\\Anaconda3\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/page2/\n"
     ]
    }
   ],
   "source": [
    "# 크롤링 할 사이트 주소\n",
    "url = 'https://pjt3591oo.github.io/'\n",
    "div_list = getDivList(url)\n",
    "# 수집된 데이터를 담을 리스트\n",
    "data_list = []\n",
    "\n",
    "while True : \n",
    "    time.sleep(1)\n",
    "    # div 태그 리스트를 가져온다\n",
    "    div_list, next_target = getDivList(url)\n",
    "    #태그의 개수만큼 반복한다\n",
    "    for div_tag in div_list:\n",
    "        tuple1 = getData(div_tag)\n",
    "        #print(tuple1)\n",
    "        # 데이터들의 이름 튜플\n",
    "        tuple2 = ('href', 'a_text', 'h4_text', 'date_text', 'author_text')\n",
    "        #print(tuple2)\n",
    "        # 딕셔너리로 만든다\n",
    "        zip1 = zip(tuple2, tuple1)\n",
    "        # print(tuple(zip1))\n",
    "        data_dic = dict(zip1)\n",
    "        # print(data_dic)\n",
    "        data_list.append(data_dic)\n",
    "    print(next_target)\n",
    "    # 다음 페이지로 이동\n",
    "    if next_target != None:\n",
    "        url = url+'{next_target}'\n",
    "    else:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
