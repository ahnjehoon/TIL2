# SPARK SQL

- 하둡을 가지고 작업할때 개발자가 전부 코드를 작성해야 했음
- HIVE라는 것을 사용하면 간단한 두세줄의 명령문이 수십줄의 코드로 변경되서 작업생산량을 늘릴 수 있음
- 이러한 HIVE를 SPARK에다가 붙여 놓은것이 SPARK SQL

---

- SQL 명령문 사용
- 함수 사용 두가지 방법으로 실습

---

## hive-site

- 분산 클러스터 환경에서 hive를활용하려면 별도 셋팅이 필요함

- 설정이나 데이터셋을 저장하는 장소의 설정은 **hive-site.xml**에 기술하고 ${SPARK_HOME}/conf에 배치함

  ```xml
  <configuration>
  	<property>
          <name>hive.metastore.warehouse.dir</name>
          <value>테이블의 입력 파일이 있는 디렉토리</value>
      </property>
  </configuration>
  ```

- 자세한 사항은 <https://cwiki.apache.org/confluence/display/Hive/AdminManual+Metastore+Administration> 참조

---

## 실습

- data 폴더 안에 있는 dessert-menu.csv 파일로 실습함

##### 스파크 SQL 초기화

- spark sql 사용을 위한 HiveContext 객체를 생성 해야함

- 생성한 SQLContext/HiveContext의 인스턴스를 sqlContext라 했을때 sqlContet.implicits._을 import해야함

  ```scala
  import org.apache.spark.{SparkConf, SparkContext}
  import org.apache.spark.sql.hive.HiveContext
  
  val conf = new SparkConf()
  val sc = new SparkContext(conf)
  val sqlContext = new HiveContext(sc)
  import sqlContext.implicits._
  ```

---

##### RDD로 부터 DataFrame 생성

- RDD가 가진 데이터에 스키마 정보를 부여함

- 그 중 가장 간단한 방법은 케이스 클래스(case class)를 이용하는 것임

  ```scala
  case class Dessert(menuId: String, name: String, price: Int, kcal: Int, menu_type:String)
  val dessertRDD = sc.textFile("spark/data/dessert-menu.csv")
  val dessertDF = dessertRDD.map{ record =>
      val sr = record.split(",")
      Dessert(sr(0), sr(1), sr(2).toInt, sr(3).toInt, sr(4))
  }.toDF
  dessertDF.printSchema
  //dessertDF.show 
  ```

  스키마 정보 출력 결과

  ```shell
  root
   |-- menuId: string (nullable = true)
   |-- name: string (nullable = true)
   |-- price: integer (nullable = false)
   |-- kcal: integer (nullable = false)
   |-- menu_type: string (nullable = true)
  
  ```

---

##### DataFrame 에 쿼리 실행

- DataFrame을 조작하는 방법은 DataFrame에 쿼리를 발행 하는 것임

- DataFrame에 직접 쿼리를 발행할 수는 없고 임시테이블에 등록해서 임시테이블로 쿼리를 발행해야함

  ```scala
  desertDF.registerTempTable("dessert_table")
  ```

- 쿼리 발행. 임시 테이블명을 대문자로 바꿔도 나오는거보니 대소문자 구별 안하는듯

  ```scala
  val numOver300KcalDF = sqlContext.sql(
  	"SELECT COUNT(*) AS NUM_OVER_300Kcal"
      +" FROM DESSERT_TABLE WHERE KCAL >= 260"
  )
   numOver300KcalDF.show
  ```

  출력 결과

  ```shell
  +----------------+
  |NUM_OVER_300Kcal|
  +----------------+
  |              12|
  +----------------+
  ```

- 스파크 내장 함수 사용

  ```scala
  // 주의: 작은 따옴표가 아니라 `임
  sqlContext.sql("SELECT atan2(1, 3) AS `ATAN2(1, 3)`").show
  ```

- 하이브 내장 함수 사용

  ```scala
  sqlContext.sql("SELECT pi() AS PI, e() AS E").show
  ```

---

##### DataFrame API로 DataFrame 다루기

- 쿼리실행은 1로 API는 2로 작성함

- 컬럼과 식 선택

  ```scala
  // 모든 데이터 선택
  val selectAllDF1 = sqlContext.sql("SELECT * FROM DESSERT_TABLE")
  val selectAllDF2 = dessertDF.select("*")
  
  // 일부 컬럼 선택
  val nameAndPriceDF1 = sqlContext.sql("SELECT name, price FROM DESSERT_TABLE")
  val nameAndPriceDF2 = dessertDF.select(dessertDF("name"), dessertDF("price"))
  // 또는
  // $는 두개 이상의 DataFrame쓸때는 사용불가능
  val nameAndPriceDF2_2 = dessertDF.select($"name", $"price")
  ```

- WHERE 조건

  ```scala
  val over5000WonDF1 = sqlContext.sql("select name from dessert_table where price >= 5000")
  // where 조건 이후에 select 사용하는것 주의
  // select를 먼저했을 경우 name 컬럼은 있는데 price컬럼이 없어서 오류남
  val over5000WonDF2 = dessertDF.where($"price" >= 5000).select($"name")
  ```

- 정렬

  ```scala
  val sortedDF1 = sqlContext.sql(
      "select name "
      +"from dessert_table "
      +"where price >= 5000"
  	+"order by price desc"
  )
  val sortedDF2 = dessertDF.where($"price" >= 5000).select($"name").orderBy($"name".desc)
  ```

- 집약처리

  ```scala
  // 300 kcal 이상인 상품 가격의 총합
  val sumDF1 = sqlContext.sql(
      "select sum(price) from dessert_table "
      + "where kcal >= 300"
  )
  val sumDF2 = dessertDF.where("kcal >= 300").agg(sum("price"))
  
  // 가격이 4000원 이상인 상품들의 평균 칼로리
  val avgDF1 = sqlContext.sql(
      "select avg(kcal) from dessert_table "
      + "where price >= 4000"
  )
  val avgDF2 = dessertDF.where("price >= 4000").agg(avg("kcal")) 
  
  // 최대 최소
  val max_minDF1 = sqlContext.sql("select max(price), min(price) from dessert_table")
  val max_minDF2 = dessertDF.agg(max("price"), min("price"))
  
  // 상품 가격이 3000원 이상인 상품의 수
  val countDF1 = sqlContext.sql(
      "select count(*) from dessert_table "
      + "where price >= 3000"
  )
  val countDF2 = dessertDF.where("price >= 3000").agg(count("*")) 
  
  // 가격 총합, 평균, 최대, 최소, 개수 group by
  val allDF1 = sqlContext.sql(
      "select sum(price), avg(price), max(price) "
      + ", min(price), count(price) "
      + "from dessert_table"
  )
  val allDF2 = dessertDF.agg(sum("price"), avg("price"), max("price"), min("price"), count("price"))
  ```

---

##### case class 만들지 않고 SPARK SQL 작성하기

- SPARK SQL을 작성하기 위해 case class를 작성해줘야하는데 이것을 생략 할 수 있음

  ```scala
  val makeRDD = sourceRDD1.map { record =>
      val sd = record.split(",")
      (sd(0), sd(1), sd(2).toInt)
  }
  val makeDF = makeRDD.toDF("colname1", "colname2", "colname3")
  ```

---

###### DataFrame끼리의 결합

- 두 개 이상의 DataFrame을 키로 결합할 수 있음

  - inner: 내부결합
  - outer: 완전 외부 결합
  - leftouter: 왼쪽 외부 결합
  - rightouter: 오른쪽 외부 결합
  - leftsemi: 왼쪽 준결합

- 여기 실습에서는 data폴더 내부의 dessert-menu.csv, dessert-order.csv 파일 두개로 실습함

  ```scala
  // 공통부분
  val conf = new SparkConf
  val sc = new SparkContext(conf)
  val sourceRDD1 = sc.textFile("spark/data/dessert-menu.csv")
  val sourceRDD2 = sc.textFile("spark/data/dessert-order.csv")
  
  // RDD 생성
  val menuRDD = sourceRDD1.map { record =>
      val sd = record.split(",")
      (sd(0), sd(1), sd(2).toInt)
  }
  val orderRDD = sourceRDD2.map { record =>
      val sd = record.split(",")
      (sd(0), sd(1), sd(2).toInt)
  }
  ```

- 방법1

  ```scala
  val joinRDD = menuRDD.join(orderRDD)
  joinRDD.foreach {
      case (id, ((name, price), (userid, amount))) =>
      println(s"${userid}는 ${name}을 ${amount}개 구매하였고 가격은 ${price.toInt * amount.toInt} 입니다")			
  }
  ```

- 방법2

  ```scala
  // SPARK SQL
  import org.apache.spark.sql.hive.HiveContext
  val sqlContext = new HiveContext(sc)
  import sqlContext.implicits._
  
  val menuDF = menuRDD.toDF("product_id", "name", "price")
  val orderDF = orderRDD.toDF("user_id", "product_id", "amount") 
  
  menuDF.registerTempTable("menu_table")
  orderDF.registerTempTable("order_table")
  
  val joinDF = sqlContext.sql(
      "select a1.name, a1.price, a2.user_id, a2.amount "
      + "from menu_table as a1, order_table as a2 "
      + "where a1.product_id = a2.product_id"
  )
  
  joinDF.show
  
  joinDF.foreach{ row =>
      val a2 = row(1).toString.toInt
      val a4 = row(3).toString.toInt
      val total = a2 * a4
      println(s"${row(2)}은 ${row(0)}를 ${row(3)}만큼 구매하였습니다")				
  }
  ```