# 기본어플리케이션 작성

------

## 개요

- 스파크는 분산처리를 기술하기 위해 **기본 API(Core API)**를 제공함
  - 생성(creation)
  - 변환(trainsformasion)
  - 액션(action)
  - RDD 영속화(persist)
  - RDD 분산처리 보조 역할(shared variable)

----

## RDD 영속화

- 영속화 저장장치 지정
  RDD를 구성하는 파티션을 메모리, 디스크 또는 두 저장장치 모두 영속화하도록 지정가능
  메모리에 영속화할때 속도는 빠른데 공간확보가 필요한 경우 오래된 캐시가 지워짐. 이거 백업하려고 디스크에 저장가능
- 직렬화(serialize) 여부 지정
  직렬화 시 영속화 저장장치의 용량을 절약가능하지만, 역속화 할 때 발생하는 직렬화 처리와 영속화한 파티션을 이용할 때 발생하는 역직렬화(desrialize) 처리와 같은 오버헤드가 단점
- 레플리케이션(replication) 여부 지정
  영속화한 파티션을 복제해서 여러개의 익스큐터(*executor*)가 갖고 있게함으로써,
  특정 익스큐터 장애가 일어나면 다른 익스큐터가 영속화한 파티션을 이용할 수 있음



------

## 공유변수

- 공유 변수는 드라이버 프로그램과 태스크가 공유하고 이용하는 것을 전제로함
- 어큐뮬레이터(accumulators)
  드라이버 프로그램에서는 값의 설정과 읽기를 수행하고, 익스큐터상에서 실행되는 태스크에서는 값의 가산만을 수행하는 공유변수
  이렇게 각 기능을 제한하면서 드라이버 프로그램과 태스크가 동시에 접근해도 변수의 내용이 바뀌지 않음
  어플리케이션에서 발생하는 이벤트를 세는 계수기(counter)와 같은 용도로 적합

- 브로드캐스트 변수(broadcast variables)
  어플리케이션 실행 중 모든 익스큐터에 단 한번만 전달되고, 이후에는 읽기 전용으로 제한되는 공유변수
  마스터 데이터등 여러개의 익스큐터에서 실행되는 변환 또는 액션처리에서 공통으로 읽어야 되므로, 갱신이 필요 없는 읽기 전용 데이터를 다루는데 적합

----

## 스파크 셸

- 개발 테스트 하기위해 클러스터 환경에 옮기는건 비효율적임(오류나면 수정해서 또올리고 반복..)

- 어플리케이션을 만들기 위해 라인 하나하나를 확인해가면서 대화식으로 처리를 기술하는게 편리함

- 구동

  ```shell
  ${SPARK_HOME}/bin/spark-shell --master local
  ```

  위의 명령은 로컬모드에서 실행하는 경우의 예이며, yarn-client로 실행시는 --master 대신 yarn-client 쓰면됨

- 텍스트 파일로 RDD 생성

  textFile 메서드에 의해 생성된 RDD의 파티션 수는 텍스트 파일의 크기를 대략 128 나눈 수의 근사치(2보다 작은 경우 파티션수가 2가 됨. 스파크에서 지정할 수 있는 최소의 파티션 수)

  ```scala
  val textRDD = sc.textFile("/path/to/simple-words.txt")
  ```

- 내부 구성 확인 collect 액션 처리 메서드
  데이터 크기가 작을 경우 편리함
  RDD에 collect를 적용하면 클러스터에서 분산처리된 해당 RDD의 파티션이 전부 드라이버 프로그램으로 반환됨
  이때, 돌려받은 파티션을 하나의 배열로 만드는데 RDD의 크기가 너무크면 OutOfMemoryError문제를 일으키므로 사용시 주의

  ```scala
  val textArray - textRDD.collect
  textArray.foreach(println)
  ```

- RDD 요소 필터링
  filter는 RDD에 포함되는 요소 중 필터링 조건에 맞는 요소만을 갖는 RDD를 생성할때 이용
  RDD 요소의 자료형을 T라 했을 때, 필터링 조건은 filter 메서드의 매개변수에 T => Boolean형 함수를 넘겨주는 것으로 설정 가능
  이 함수는 RDD의 각 요소를 매개변수로 filter 메서드의 내부에서 호출
  매개변수로 넘겨받은 문자열이 단어인지 아닌지 판정하는 함수 isWord 예제

  ```scala
  val isWord: String => Boolean = word => word.matches("""\p{Alnum}+""")
  val wordRDD = textRDD.filter(isWord)
  ```

- 함수 리터럴(function literal)
  위의 isWord를 별도로 만들지 않고 처리할수 있음
  이름이 없는 함수의 정의를 **함수 리터럴**이라고 함

  ```scala
  val wordRDD = textRDD.filter(word => word.matches("""\p{Alnum}+"""))
  // 플레이스 홀더(place holder)를 사용해서 더 간단하게
  // 언더스코어 기호는 함수에 넘겨주는 매개변수를 나타냄
  // 언더스코어를 여러 개 이용하는 경우 첫 번째 언더스코어가 함수에 넘겨지는 첫 번째 배개변수가 되고
  // 두 번째 언더스코어가 두 번째 매개변수로 인식됨
  val wordRDD = textRDD.filter(_.matches("""\p{Alnum}+"""))
  // 이 함수를 이용해서 문자열만 출력
  val wordArray = wordRDD.collect
  wordArray.foreach(println)
  ```

- RDD 요소 가공해서 단어 세기
  위의 예제에서 wordRDD를 가지고 각 단어가 등장하는 횟수를 셀때의 단계는 다음과 같다.
  먼저 wordRDD에 포함되는 단어를 바탕으로 (단어, 1)형의 튜플을 요소로 가진 RDD를 생성하고
  튜플을 기반으로 같은 단어를 가진 요소를 모아 값을 처리하면 단어의 등장횟수를 셀 수 있음

  ```scala
  val wordAndOnePairRDD = wordRDD.map(word => (word, 1))
  val wordAndCountRDD = wordAndOnePairRDD.reduceByKey((result, elem) => result + elem)
  val wordAndOnePairArray = wordAndCountRDD.collect
  wordAndOnePairArray.foreach(println)
  ```

  키 단위로 집양처리가 실행 될때 셔플처리도 발생함(집약처리를 위해서는 동일 키의 요소들이 같은 파티션에 있어야 하기 때문)
  reduceByKey 메서드를 호출한 경우에는 잡이 실행되는 시점에 다대다 네트워크 통신이 발생

- 다중구조 컬렉션(nested collection)을 1차원 배열로 만들기

  ```scala
  val textRDD = sc.textFile("/README.md")
  val wordCandidataRDD = textRDD.flatMap(_.split("[ ,.]"))
  val wordRDD = wordCandidataRDD.filter(_.matches("""\p{Alnum}+"""))
  val wordAndOnePairRDD = wordRDD.map((_, 1))
  val wordAndCountRDD = wordAndOnePairRDD.reduceByKey(_ + _)
  val wordAndCountArray = wordAndCountRDD.collect
  wordAndCountArray.foreach(println)
  ```

---

# 드라이버 프로그램 작성

- 스파크 셸에서 처리를 기술할 떄는 스파크 셸 자신이 드라이버 프로그램으로 동작했음

- 하지만 어플리케이션을 개발 할때는 드라이버 프로그램을 만들어야 함

- 드라이버 프로그램은 스칼라 객체로 기술하고, 앤트리 포인트는 main 메서드로 기술

- 스파크 셸을 이용했을 때는 SparkContext가 자동 생성 되었지만, 직접 드라이버 프로그램을 만드는 경우에는 명시적으로 생성해줘야함

- SparkContext생성전 SparkConf(어플리 케이션 설정을 담당하는 클래스)를 먼저 생성해야함(spark-shell에서는 try안에 있는것만 실행해도됨)

- 기본 형태는 아래와 같음

  ```scala
  package com.example.test
  
  import org.apache.spark.{SparkConf, SparkContext}
  
  object sampleProject{
      def main(args: Array[String]){
          val conf = new SparkConf
          // 아래와 같이 이름을 설정 할 수 있음
          // 이 경우 spark-submit 커맨드 실행시 --name 옵션이 필요가 없음
          //val conf = new SparkConf()setAppName("sampleProject")
          val sc = new SparkContext(conf)
          try{
  	        // 이곳에 코드 작성
          } finally {
              sc.stop()
          }
      }
  }
  ```

------

# sbt로 스파크 애플리케이션 실행하기

- 스파크 어플리케이션은 소스코드를 컴파일하고 JAR파일로 패키징 해야함
- 여기서는 sbt를 이용해 컴파일과 패키징을 실행함
- sbt는 스칼라와 자바로 기술된 소스코드를 컴파일, 라이브러리 의존 관계를 관리하며 패키지를 작성하는 등 빌드 프로세스를 위한 툴임
- sbt 쓰려면 JRE 또는 JDK를 설치해야 함
- 인터넷으로 외부 repository 라이브러리를 내려받으므로 인터넷에 연결되어 있어야함

##### 구성

- sbt로 어플리케이션을 빌드하려면 build.sbt 파일을 작성하고 '빌드 정의(build definition)'를 설정해야함

- 또한 어셈플리 JAR 파일을 작성하려면 추가로  sbt-assembly 플러그인 필요

- build.sbt

  - 프로젝트의 루트 디렉토리에 작성
  - 키 - 밸류 형식으로 빌드 정의
  - name = 프로젝트 이름
  - version = 프로젝트 버전
  - scalaVersion = 스칼라 버전(작성중일 때의 버전 2.10.4)
  - libraryDependencies 
    - 의존하는 라이브러리가 구체적으로 무엇인지 설정
    - 해당 라이브러리가 빌드 과정중 어느 시점에 의존할지 설정(컴파일 또는 패키징할 때)
    - 의존 라이브러리는 "<groupID>" % "<artifactID>" % "<version>" % "<configuration>" 형식으로 기술
    - 의존 라이브러리를 관리하는 repository 사이트에서 검색 가능 [maven repository](<https://mvnrepository.com/>)
    - 아래 예시의 경우 Spark 기본 라이브러리와 날짜처리를 위한 joda-time을 의존 라이브로 설정한다는 뜻
    - 종류
      - 스파크 코어
        libraryDependencies ++= Seq("org.spache.spark" % "spark-core_2.10" % "1.6.1" % "provided")
      - 스파크 SQL
        libraryDependencies ++= Seq("org.spache.spark" % "spark-hive_2.10" % "1.6.1" % "provided")
      - 스파크 스트리밍
        libraryDependencies ++= Seq("org.spache.spark" % "spark-streaming_2.10" % "1.6.1" % "provided")
      - Mlib
        libraryDependencies ++= Seq("org.spache.spark" % "spark-mlib_2.10" % "1.6.1" % "provided")
      - 그래프 X
        libraryDependencies ++= Seq("org.spache.spark" % "spark-graphx_2.10" % "1.6.1" % "provided")
  - assemblyOption in assembly = sbt-assembly 플러그인의 옵션

  예시

  ```
  name := "sampleProject"
  version := "0.1"
  scalaVersion := "2.10.4"
  libraryDependencies ++=
  	Seq("org.spache.spark" % "spark-core_2.10" % "1.6.1" % "provided", "joda-time" % "joda-time" % "2.8.2")
  assemblyOption in assembly := (assemblyOption in assembly).value.copy(includeScala = false)
  ```

- plugins.sbt 작성

  - sbt-assembly 플러그인을 이용하려면 plugins.sbt를 project 디렉터리에 작성해야함
  - plugins.sbt 내용
    addSbtPlugin("com.eed3si9n" % "sbt-assembly" % "0.13.0")



##### 실행

- 스파크 어플리 케이션을 실행할 때는 spark-submit 명령을 이용하는데 형식은 아래와 같음

  ```
  ${SPARK_HOME}/bin/spark-submit \
  --master <동작 모드> \
  --class <main 메소드가 구현된 어플리케이션의 패키지명.클래스명> \
  --name <프로젝트명> \
  어플리 케이션의 클라스가 포함된 JAR 파일 \
  <어플리케이션에 넘기는 옵션, 인자값들>
  ```

- 예시

  하위 디렉토리 작성

  1. cd ~

  2. mkdir spark-simple-app

  3. cd spark-simple-app

  4. mkdir -p src/main/scala
     프로젝트의 루트 디렉토리 아래에 스칼라 소스코드를 두는 디렉토리 작성

  5. mkdir project
     sbt 관련 설정 파일을 두는 디렉토리 작성

  6. cd ~/spark-simple-app

  7. ```shell
     spark-submit --master local 
                  --class com.example.test.sampleProject
                  --name sampleProject
     	     target/scala-2.10/ProductSales-assembly-0.1.jar
     ```

##### local 모드

- spark-submit 명령을 실행한 클라이언트상에서 프로세스를 구동하고 해당 프로세스 안에서 익스큐터를 구동하여 실행하는 동작모드
- 옵션
  - local
    익스큐터에 한 개의 스레드만을 할당
  - local[*]
    클라이언트 머신에 탑재된 CPU의 코어 수만큼 익스큐터에 스레드 할당
  - local[<스레드개수>]
    익스큐터에 지정된 수만큼 스레드 할당

##### yarn-client 모드 / yarn-cluster 모드

- YARN이 관리하는 클러스터상에서 어플리케이션을 실행하는 모드
- 클러스터의 노드 매니저상에서 익스큐터가 동작하고, 익스큐터가 분산처리를 시행함
- --master뒤에 어느 모드로 할껀지 정해주면 됨
- yarn-client 모드
  - 클라이언트 호스트상에서 동작
  - 드라이버 프로그램의 표준 출력 내용을 확인해가면서 어플리케이션을 디버깅할때 도움
- yarn-cluster
  - 클러스터의 NodeManger상에서 동작
  - 클러스터 전체의 계산 리소스를 이용할 수 있고 오류 처리고 YARN에 위임가능

##### spark-submit 옵션 목록

- --jars <JAR파일, ...>
  - 클라이언트상의 JAR 파일 경로를 쉼표로 구분해서 지정
- --files <클라이언트상의 파일 경로>
  - 익스큐터에 파일 배포
  - 복수의 파일을 배포하는 경우 쉼표로 구분
  - 배포된 파일은 각 익스큐터의 작업디렉토리에 보관
- --conf 속성이름 = 값
  - 지정되는 프로퍼티를 애플리케이션 실행 시 동적으로 설정
    spark. 으로 시작되는 프로퍼티만 지정가능
- --properties-file
  - 스파크 설정이 기술된 파일의 경로를 지정
  - --conf로 일일이 지정하는 대신 파일에 여러개의 프로퍼티를 지정하고 그 파일을 읽는 방법
- --driver-memory <메모리 크기>
  - 드라이버에 할당하는 메모리 크기 지정(기본 1024M)
- --executor-memory <메모리 크기>
  - 익스큐터 한 개에 할당되는 메모리 크기 지정(기본 1024M)
- YARN 클러스터 상에서 실행할 경우 드라이버 프로그램과 익스큐터에 할당할 CPU코어 수와 익스큐터 수 설정 가능함
  - --driver-cores <CPU 코어 수>
    - 드라이버에 할당하는 CPU 코어 수 지정. yarn-cluster 모드에만 유효(기본 1)
  - --executor-cores <CPU 코어 수>
    - 익스큐터에 할당하는 PCU 코어의 수(기본 1)
  - --num-executors <익스큐터 개수>
    - 구동할 익스큐터의 개수 지정
- 그외 spark-submit --help 참조